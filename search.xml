<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ORACLE锁表问题处理]]></title>
    <url>%2F2020%2F12%2F18%2Foracle%E9%94%81%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[for update造成oracle锁表问题解决今天下午在单元测试造数据过程中在pl/sql中使用select * from table for update 修改数据 1select * from T_BS_NEXGO_MERCH_PIC t where t.APPLY_ID='fuiou112020081218312000 for update; 当数据修改完成后没有进行commit就关掉了PL/SQL，之后再插入数据时PL/SQL直接卡死。 检查是否锁表查看是否锁表的sql 1select * from v$session t1, v$locked_object t2 where t1.sid = t2.SESSION_ID; 执行锁表语句结果 从而发现有一条锁表记录 解锁杀掉锁表进程：记录下SID和serial# ，分别替换掉下面的1900,11073，即可解除锁表，注意，必须有管理员权限才可以执行 1alter system kill session '1900,11073'; 后记以后如果在plsql中不想用SQL语句修改数据，直接在表中修改数据避免使用for update,多用如下语句1select t.*,rowid from tbname 以上语句同样可以在plsql的查询结果中尽心数据修改，并且可避免锁表风险。]]></content>
      <categories>
        <category>ORACLE</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>ORACLE</tag>
        <tag>锁表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot https访问配置]]></title>
    <url>%2F2020%2F11%2F11%2Fspringboot-https%2F</url>
    <content type="text"><![CDATA[好久好久没有写这个博客了，如果今天不是为了给域名续费我都忘了我还有个博客]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查看表注释或字段注释]]></title>
    <url>%2F2019%2F03%2F07%2F%E6%9F%A5%E7%9C%8B%E8%A1%A8%E6%B3%A8%E9%87%8A%E6%88%96%E5%AD%97%E6%AE%B5%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[查看所有表的注释123456789SELECTtable_name 表名,table_comment 表说明FROMinformation_schema.TABLESWHEREtable_schema = '数据库名'ORDER BYtable_name 查询所有表及字段的注释1234567891011121314SELECTa.table_name 表名,a.table_comment 表说明,b.COLUMN_NAME 字段名,b.column_comment 字段说明,b.column_type 字段类型,b.column_key 约束FROMinformation_schema. TABLES aLEFT JOIN information_schema. COLUMNS b ON a.table_name = b.TABLE_NAMEWHEREa.table_schema = '数据库名'ORDER BYa.table_name 查询某表的所有字段的注释1234567selectCOLUMN_NAME 字段名,column_comment 字段说明,column_type 字段类型,column_key 约束 from information_schema.columnswhere table_schema = '数据库名'and table_name = '表名' ; 或者1show full columns from 表名; 查看表生成的DDL注意表名不加单引号1show create table 表名; 新建表以及添加表和字段的注释12345create table t_user( ID INT(19) primary key auto_increment comment '主键', NAME VARCHAR(300) comment '姓名', CREATE_TIME date comment '创建时间')comment = '用户信息表'; 修改表/字段的注释修改表注释1alter table t_user comment = '修改后的表注释信息(用户信息表)'; 修改字段注释1alter table t_user modify column id int comment '主键ID';]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[confluence6.9安装]]></title>
    <url>%2F2018%2F12%2F28%2Fconfluence6-9%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[简介 Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。 Confluence为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。 环境要求本次安装已CentOS7.4为例，安装前需要mysql，jdk支持。 运行环境 CentOS7.4 mysql version 5.7.20 jdk java version “1.8.0_191 Confluence安装下载下载安装包和注册工具1234# 安装包下载 wget https://product-downloads.atlassian.com/software/confluence/downloads/atlassian-confluence-6.9.1-x64.bin# 注册工具下载 wget http://down.whsir.com/downloads/confluence_crack.zip 设置执行权限1chmod +x atlassian-confluence-6.9.1-x64.bin 安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ ./atlassian-confluence-6.9.1-x64.binUnpacking JRE ...Starting Installer ...You do not have administrator rights to this machine and as such, some installation options will not be available. Are you sure you want to continue?Yes [y, Enter], No [n]输入y或者直接回车yThis will install Confluence 6.9.1 on your computer.OK [o, Enter], Cancel [c]# 输入o或直接回车oClick Next to continue, or Cancel to exit Setup.Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (uses default settings) [1],Custom Install (recommended for advanced users) [2, Enter],Upgrade an existing Confluence installation [3]# 这里输入数字11See where Confluence will be installed and the settings that will be used.Installation Directory: /home/xhc/atlassian/confluenceHome Directory: /home/xhc/atlassian/application-data/confluenceHTTP Port: 8090RMI Port: 8000Install as service: NoInstall [i, Enter], Exit [e]# 输入i或者直接回车iExtracting files ...Please wait a few moments while we configure Confluence.Installation of Confluence 6.9.1 is completeStart Confluence now?Yes [y, Enter], No [n]# 输入y或者直接回车yPlease wait a few moments while Confluence starts up.Launching Confluence ...Installation of Confluence 6.9.1 is completeYour installation of Confluence 6.9.1 is now ready and can be accessed viayour browser.Confluence 6.9.1 can be accessed at http://localhost:8090Finishing installation ... 安装完成会自动启动 打开网页获取ID访问http://IP:8090 切换语言 获得插件 授权码走到这一步，就可获取服务器id 破解破解需要分两步： 一是破解文件 二是获取授权码 破解文件 从服务器上下载/opt/atlassian/confluence/confluence/WEB-INF/lib/atlassian-extras-decoder-v2-3.3.0.jar文件到windows客户端重命名为atlassian-extras-2.4.jar 运行confluence_keygen.jar（文中最开始的那个破解包，在windows上运行，需要在windows上安装好java） 选择.patch!找到刚才重命名的那个文件打开 打开后在当前目录下可以看到atlassian-extras-2.4.jar和atlassian-extras-2.4.bak两个文件，这里atlassian-extras-2.4.jar已经是破解好的了，将atlassian-extras-2.4.jar名字改回来atlassian-extras-decoder-v2-3.3.0.jar上传到服务器上的/opt/atlassian/confluence/confluence/WEB-INF/lib/目录，覆盖原来的atlassian-extras-decoder-v2-3.3.0.jar 二、获取授权码复制网页中的服务器ID，运行破解工具confluence_keygen.jar，破解复制Key到Confluence里，获得授权码，进行下一步 数据库配置 参考文章 http://blog.51cto.com/m51cto/2131964 https://blog.csdn.net/weixin_41004350/article/details/80590421]]></content>
      <categories>
        <category>confluence</category>
      </categories>
      <tags>
        <tag>confluence</tag>
        <tag>wiki</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx安装配置]]></title>
    <url>%2F2018%2F11%2F19%2Fnginx%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境 Linux版本：Linux version 3.8.13-26.1.1.el6uek.x86_64 (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) Nginx版本：nginx-1.13.12 安装依赖软件12yum -y install gcc gcc-c++ autoconf automake makeyum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel 安装 Nginx下载1# wget http://nginx.org/download/nginx-1.13.12.tar.gz 解压1# tar -xvf nginx-1.13.12.tar.gz 创建安装目录1# mkdir -p /usr/local/nginx 修改配置12# cd nginx-1.13.12/# ./configure --prefix=/usr/local/nginx 安装1#make &amp;&amp; make install 启动进入安装目录1# cd /usr/local/nginx/sbin 启动1# ./nginx 这时候在安装机器就可以输入地址查看了。 如果远程访问的话需要关闭防火墙或者将80端口开放，添加新端口后需要reload 防火墙。 Nginx 其他命令以下包含了 Nginx 常用的几个命令：12345./nginx -s reload # 重新载入配置文件./nginx -s reopen # 重启 Nginx./nginx -s stop # 停止 Nginx./nginx -t # 测试配置文件./nginx -v # 查看版本]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nginx</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch安装使用]]></title>
    <url>%2F2018%2F11%2F10%2FElasticsearch%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文以目前最新版本elasticsearch-6.4.3为例 下载elasticsearch官方下载：https://www.elastic.co/cn/downloads/elasticsearch1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.3.tar.gz 解压1tar -xvf elasticsearch-6.4.3.tar.gz 单例安装注意:elasticsearch安装时不能以root权限进行安装，否则会报以下错误12345678910111213141516[root@kiko1 bin]# ./elasticsearch[2018-11-10T17:41:55,713][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-6.4.3.jar:6.4.3] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[elasticsearch-6.4.3.jar:6.4.3]Caused by: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:104) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:171) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[elasticsearch-6.4.3.jar:6.4.3] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[elasticsearch-6.4.3.jar:6.4.3] ... 6 more 创建专有用户123456## root权限下新增一个用户useradd kiko#授权chown -R kiko /home/kiko/tools/elasticsearch-*## 进入bin目录下进行安装[kiko@kiko1 bin]$ ./elasticsearch 启动1[kiko@kiko1 bin]$ ./elasticsearch &amp; 测试1[kiko@kiko1 config]$ curl localhost:9200 响应结果1234567891011121314151617&#123; &quot;name&quot; : &quot;2124wwq&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;Ir233ETeSx2uSTfEqJ5Egw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.4.3&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;fe40335&quot;, &quot;build_date&quot; : &quot;2018-10-30T23:17:19.084789Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.4.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 配置 elasticsearch.ymlelasticsearch-6.4.3/config/elasticsearch.yml123456## 允许外网访问network.host: 0.0.0.0##允许跨域访问，方便后面可视化工具elasticsearch-headhttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 配置后重启发现报错,并且不能重启成功12345678[2018-11-21T19:52:41,151][INFO ][o.e.b.BootstrapChecks ] [2124wwq] bound or publishing to a non-loopback address, enforcing bootstrap checksERROR: [2] bootstrap checks failed[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144][2018-11-21T19:52:41,221][INFO ][o.e.n.Node ] [2124wwq] stopping ...[2018-11-21T19:52:41,267][INFO ][o.e.n.Node ] [2124wwq] stopped[2018-11-21T19:52:41,268][INFO ][o.e.n.Node ] [2124wwq] closing ...[2018-11-21T19:52:41,285][INFO ][o.e.n.Node ] [2124wwq] closed 解决方法，用root权限修改limits.conf配置1vim /etc/security/limits.conf 查找到下配置参数并修改为以下值1234* soft nproc 2048* hard nproc 4096* soft nofile 65536* hard nofile 131072 1vim /etc/sysctl.conf sysctl.conf 最后一行添加如下配置1vm.max_map_count = 655360 然后使其生效1sysctl -p 保存后重新用kiko用户启动./elasticsearch &amp; 启动完毕之后，打开浏览器输入链接：http://192.168.229.115:9200 分布式安装主节点配置12345vim config/elasticsearch.yml#添加一下配置cluster.name: kobenode.name: masternode.master: true 重启服务。 从节点1配置如果是一台服务器，从新解压一份到slave1文件夹 12345678vim config/elasticsearch.ymlcluster.name: kobenode.name: slave1http.port: 9800network.host: 0.0.0.0#设置主动发现主节点地址discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] 如果是在另外一台服务器，按照单例安装方式解压配置安装后1234567vim config/elasticsearch.ymlcluster.name: kobenode.name: slave1network.host: 0.0.0.0#设置主动发现主节点地址discovery.zen.ping.unicast.hosts: [&quot;192.168.229.115&quot;] #这里是主节点]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>es</tag>
        <tag>Elasticsearch</tag>
        <tag>搜索</tag>
        <tag>Elasticsearch安装</tag>
        <tag>安装</tag>
        <tag>solr</tag>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis单机版安装]]></title>
    <url>%2F2018%2F09%2F18%2Fredis-signle-install%2F</url>
    <content type="text"><![CDATA[环境 环境 版本 linux Red Hat Enterprise Linux Server release 6.10 (Santiago) redis 3.2.12 下载在redis官网网站去下载,具体下载地址12345678910111213[root@kiko1 tools]# wget http://download.redis.io/releases/redis-3.2.12.tar.gz[root@kiko1 tools]# tar -xvf redis-3.2.12.tar.gz[root@kiko1 tools]# cd redis-3.2.12[root@kiko1 redis-3.2.12]# make[root@kiko1 redis-3.2.12]# cd src &amp;&amp; make install[root@kiko1 redis-3.2.12]# cd utils[root@kiko1 utils]# ./install_server.sh 初始化设置1234567891011121314151617181920Welcome to the redis service installerThis script will help you easily set up a running redis serverPlease select the redis port for this instance: [6379]Selecting default: 6379Please select the redis config file name [/etc/redis/6379.conf]Selected default - /etc/redis/6379.confPlease select the redis log file name [/var/log/redis_6379.log]Selected default - /var/log/redis_6379.logPlease select the data directory for this instance [/var/lib/redis/6379]Selected default - /var/lib/redis/6379Please select the redis executable path [/usr/local/bin/redis-server]Selected config:Port : 6379Config file : /etc/redis/6379.confLog file : /var/log/redis_6379.logData dir : /var/lib/redis/6379Executable : /usr/local/bin/redis-serverCli Executable : /usr/local/bin/redis-cliIs this ok? Then press ENTER to go on or Ctrl-C to abort.]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>redis</tag>
        <tag>redis3.2.9</tag>
        <tag>redis安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAVEN加载本地jar包]]></title>
    <url>%2F2018%2F07%2F02%2Fmaven-local-jar%2F</url>
    <content type="text"><![CDATA[正常情况下，在以MAVEN创建的java工程中，引用jar包直接在本地Maven仓库或远程仓库中找到响应jar的maven依赖，直接添加到pom.xml的&lt;dependencies&gt;...&lt;/dependencies&gt;中。 但最近在做第三方支付服务，该服务需要引入支付有关的几个jar包。而这些jar包在maven官方仓库中是没有的。 那么如何在添加本地的jar包到maven中并可以package成可执行jar包呢？ 配置MAVEN环境变量maven环境配置不在此赘述。 方法一 将本地jar包添加到本地仓库添加本地jar包到本地Maven仓库Maven安装jar包命令是1mvn install:install-file -Dfile=jar包的位置 -DgroupId=自定义groupId -DartifactId=自定义artifactId -Dversion=自定义version -Dpackaging=jar 例如：12mvn install:install-file -Dfile=F:\svn\hsxt-dev\09_20180605\hsxt-access-web\hsxt-access-web-aps\lib\commons-utils-0.0.1-SNAPSHOT.jar -DgroupId=commons-utils -DartifactId=commons-utils -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar 在java工程pom.xml文件中添加依赖123456&lt;!-- 智能POS jar--&gt;&lt;dependency&gt; &lt;groupId&gt;commons-utils&lt;/groupId&gt; &lt;artifactId&gt;commons-utils&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; package JAR包1mvn package 到此，package JAR包的方法跟平常一样。clean后package，在target目录中就会生成打包好的jar包。 方法二 将本地jar包添加到工程添加jar到工程主目录的lib中在工程主目录创建lib文件夹，然后将相应的jar包添加到lib文件夹下。 在pom.xml中添加依赖如下12345678 &lt;!-- 智能POS jar--&gt;&lt;dependency&gt; &lt;groupId&gt;commons-utils&lt;/groupId&gt; &lt;artifactId&gt;commons-utils&lt;/artifactId&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;systemPath&gt;$&#123;basedir&#125;\lib\commons-utils-0.0.1-SNAPSHOT.jar&lt;/systemPath&gt;&lt;/dependency&gt; 说明： groupId、artifactId、version任意填写。 scope表示从本地文件系统拿，一定需要配合systemPath属性使用。 systemPath为jar包的绝对路径 ${basedir}为项目根目录的绝对路径 注意事项！！ 用方法二添加jar是可以在本地将第三方jar包添加到maven工程中的。但是在package时，这些第三方jar包不会被打入到jar包中。所以该方法只适用于本地开发。 如果想要用jar包部署项目，请参见上面方法一，将jar包安装到本地maven仓库。 本文参考]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 搭建 SolrCloud 集群服务]]></title>
    <url>%2F2018%2F05%2F09%2Flinux-solr-SolrCloud%2F</url>
    <content type="text"><![CDATA[概述Lucene是一个Java语言编写的利用倒排原理实现的文本检索类库 Solr是以Lucene为基础实现的文本检索应用服务。Solr部署方式有单机方式、多机Master-Slaver方式、Cloud方式。 SolrCloud是基于Solr和Zookeeper的分布式搜索方案。当索引越来越大，一个单一的系统无法满足磁盘需求，查询速度缓慢，此时就需要分布式索引。在分布式索引中，原来的大索引，将会分成多个小索引，solr可以将这些小索引返回的结果合并，然后返回给客户端。 特色功能SolrCloud有几个特色功能： 集中式的配置信息使用ZK进行集中配置。启动时可以指定把Solr的相关配置文件上传 Zookeeper，多机器共用。这些ZK中的配置不会再拿到本地缓存，Solr直接读取ZK中的配置信息。配置文件的变动，所有机器都可以感知到。另外，Solr的一些任务也是通过ZK作为媒介发布的。目的是为了容错。接收到任务，但在执行任务时崩溃的机器，在重启后，或者集群选出候选者时，可以再次执行这个未完成的任务。 自动容错SolrCloud对索引分片，并对每个分片创建多个Replication。每个 Replication都可以对外提供服务。一个Replication挂掉不会影响索引服务。更强大的是，它还能自动的在其它机器上帮你把失败机器上的索引Replication重建并投入使用。 近实时搜索立即推送式的replication（也支持慢推送）。可以在秒内检索到新加入索引。 查询时自动负载均衡SolrCloud索引的多个Replication可以分布在多台机器上，均衡查询压力。如果查询压力大，可以通过扩展机器，增加Replication来减缓。 自动分发的索引和索引分片发送文档到任何节点，它都会转发到正确节点。 事务日志确保更新无丢失，即使文档没有索引到磁盘。 其它值得一提的功能有： 索引存储在HDFS上索引的大小通常在G和几十G，上百G的很少，这样的功能或许很难实用。但是，如果你有上亿数据来建索引的话，也是可以考虑一下的。我觉得这个功能最大的好处或许就是和下面这个“通过MR批量创建索引”联合实用。 通过MR批量创建索引有了这个功能，你还担心创建索引慢吗？ 强大的RESTful API通常你能想到的管理功能，都可以通过此API方式调用。这样写一些维护和管理脚本就方便多了。 优秀的管理界面主要信息一目了然；可以清晰的以图形化方式看到SolrCloud的部署分布；当然还有不可或缺的Debug功能。 SolrCloud的基本概念Cluster集群：一组Solr节点，逻辑上作为一个单元进行管理，整个集群使用同一套Schema和SolrConfig Node节点：一个运行Solr的JVM实例 Collection：在SolrCloud集群中逻辑意义上的完整的索引，常常被划分为一个或多个Shard。这些Shard使用相同的config set,如果Shard数超过一个，那么索引方案就是分布式索引。 Core：也就是Solr Core，一个Solr中包含一个或者多个SolrCore，每个Solr Core可以独立提供索引和查询功能，Solr Core额提出是为了增加管理灵活性和共用资源。SolrCloud中使用的配置是在Zookeeper中的，而传统的Solr Core的配置文件是在磁盘上的配置目录中。 Config Set:Solr Core提供服务必须的一组配置文件，每个Config Set有一个名字。必须包含solrconfig.xml和schema.xml，初次之外，依据这两个文件的配置内容，可能还需要包含其他文件。Config Set存储在Zookeeper中，可以重新上传或者使用upconfig命令进行更新，可以用Solr的启动参数bootstrap_confdir进行初始化或者更新。 Shard分片：Collection的逻辑分片。每个Shard被分成一个或者多个replicas，通过选举确定那个是Leader。 Replica：Shard的一个拷贝。每个Replica存在于Solr的一个Core中。 Leader：赢得选举的Shard replicas，每个Shard有多个replicas，这几个Replicas需要选举确定一个Leader。选举可以发生在任何时间。当进行索引操作时，SolrCloud将索引操作请求传到此Shard对应的leader，leader再分发它们到全部Shard的replicas。 Solr 文档 Apache SolrCloud 参考指南 Apache Solr文档 Solr 参数配置 Solr控制脚本参考 环境 Linux版本：Linux version 3.8.13-26.1.1.el6uek.x86_64 (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) Solr版本：solr-6.6.3 Jdk环境：1.8.0_121 环境配置具体参考 Linux下JDK环境变量配置 ZooKeeper版本：zookeeper-3.4.12.tar.gz 具体参考 Linux搭建 ZooKeeper-3.4.12 Cluster 集群服务注意事项关闭防火墙 iptables防火墙关闭命令1$ service iptables stop # 关闭命令： firewall防火墙关闭命令1$ systemctl stop firewalld.service # 停止firewall Solr 6（和SolrJ客户端库）的Java支持的最低版本现在是Java 8。 Solr 安装下载 Solr下载最新版本的Solr1$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/6.6.3/solr-6.6.3.zip 提取tar文件提取zip文件12$ unzip -xvf solr-6.6.3.zip$ cd solr-6.6.3 集群配置编辑 solr.in.sh集群中的每台机器都要按照以下说明进行配置启动 首先到 solr 安装目录的 bin 下,编辑 solr.in.sh 文件 搜索 SOLR_HOST, 取消注释, 设置成自己的 ip 或机器名 例如(kiko1) 建议使用机器名 搜索 SOLR_TIMEZONE, 取消注释, 设置成 UTC+8 把kiko1 的solr.in.sh 修改为以下配置 建议设置Solr服务器的主机名，特别是在以SolrCloud模式运行时，因为它会在使用ZooKeeper注册时确定节点的地址 ，不建议用ip12SOLR_HOST=&quot;kiko1&quot; #节点(机器)名称SOLR_TIMEZONE=&quot;UTC+8&quot; #时区 UTC+8为东八区 复制 Solr 配置 把 kiko1 编辑好的 Solr 文件及配置通过 scp -r 复制到集群 kiko2, kiko3 1[root@kiko1 solr-6.6.3]# for a in &#123;2..3&#125; ; do scp -r /home/kiko/tools/solr-6.6.3/ kiko$a:/home/kiko/tools/solr-6.6.3 ; done 然后依次修改 kiko2, kiko3 的上的 solr.in.sh 的SOLR_HOST 为机器的ip或机器名（建议为机器名） 格式 SOLR_HOST=”ip”1[root@kiko2 tools]# vim solr-6.6.3/bin/solr.in.sh 启动 ZooKeeper 集群1[root@kiko1 bin]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/zookeeper-3.4.12/bin/zkServer.sh start&quot; ; done 启动 SolrCloud 集群在任意一台机器，启动 SolrCloud 集群 并且关联 ZooKeeper 集群1[root@kiko1 solr-6.6.3]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/solr-6.6.3/bin/solr start -cloud -z kiko1:2181,kiko2:2181,kiko3:2181 -p 8983 -force&quot; ; done 响应 123456789101112131415Warning: Available entropy is low. As a result, use of the UUIDField, SSL, or any other features that requireRNG might not work properly. To check for the amount of available entropy, use &apos;cat /proc/sys/kernel/random/entropy_avail&apos;.Waiting up to 180 seconds to see Solr running on port 8983 [\]Started Solr server on port 8983 (pid=10860). Happy searching! Warning: Available entropy is low. As a result, use of the UUIDField, SSL, or any other features that requireRNG might not work properly. To check for the amount of available entropy, use &apos;cat /proc/sys/kernel/random/entropy_avail&apos;.Waiting up to 180 seconds to see Solr running on port 8983 [\]Started Solr server on port 8983 (pid=24326). Happy searching! Warning: Available entropy is low. As a result, use of the UUIDField, SSL, or any other features that requireRNG might not work properly. To check for the amount of available entropy, use &apos;cat /proc/sys/kernel/random/entropy_avail&apos;.Waiting up to 180 seconds to see Solr running on port 8983 [\]Started Solr server on port 8983 (pid=9689). Happy searching! 创建集群库在任意一台机器（solr节点服务器kiko1，kiko2，kiko3）执行如下命令1[root@kiko1 solr-6.6.3]# /home/kiko/tools/solr-6.6.3/bin/solr create_collection -c test_collection -shards 2 -replicationFactor 3 -force 执行命令参数说明： -c 指定库(collection)名称 -shards 指定分片数量,可简写为 -s ,索引数据会分布在这些分片上 -replicationFactor 每个分片的副本数量,每个碎片由至少1个物理副本组成 响应123456789101112131415161718192021222324252627Connecting to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181 ...INFO - 2018-05-09 20:23:42.323; org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider; Cluster at kiko1:2181,kiko2:2181,kiko3:2181 readyUploading /home/kiko/tools/solr-6.6.3/server/solr/configsets/data_driven_schema_configs/conf for config test_collection to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181Creating new collection &apos;test_collection&apos; using command:http://kiko1:8983/solr/admin/collections?action=CREATE&amp;name=test_collection&amp;numShards=2&amp;replicationFactor=3&amp;maxShardsPerNode=2&amp;collection.configName=test_collection&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:6132&#125;, &quot;success&quot;:&#123; &quot;kiko3:8983_solr&quot;:&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:4315&#125;, &quot;core&quot;:&quot;test_collection_shard1_replica1&quot;&#125;, &quot;kiko2:8983_solr&quot;:&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:4637&#125;, &quot;core&quot;:&quot;test_collection_shard1_replica2&quot;&#125;, &quot;kiko1:8983_solr&quot;:&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:4801&#125;, &quot;core&quot;:&quot;test_collection_shard1_replica3&quot;&#125;&#125;&#125; SolrCloud状态 图表 可以看到 solr 2个分片,个3个副本 服务状态如果您不确定SolrCloud状态1[root@kiko2 tools]# /home/kiko/tools/solr-6.6.3/bin/solr status 响应12345678910111213Found 1 Solr nodes:Solr process 24326 running on port 8983&#123; &quot;solr_home&quot;:&quot;/home/kiko/tools/solr-6.6.3/server/solr&quot;, &quot;version&quot;:&quot;6.6.3 d1e9bbd333ea55cfa0c75d324424606e857a775b - sarowe - 2018-03-02 15:09:34&quot;, &quot;startTime&quot;:&quot;2018-05-09T12:21:22.783Z&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 26 minutes, 40 seconds&quot;, &quot;memory&quot;:&quot;99.4 MB (%20.3) of 490.7 MB&quot;, &quot;cloud&quot;:&#123; &quot;ZooKeeper&quot;:&quot;kiko1:2181,kiko2:2181,kiko3:2181&quot;, &quot;liveNodes&quot;:&quot;3&quot;, &quot;collections&quot;:&quot;1&quot;&#125;&#125; 删除集群库在任意一台机器（solr节点服务器kiko1，kiko2，kiko3）执行命令 ./solr delete -c &lt;collection&gt; 将检查12/home/kiko/tools/solr-6.6.3/server/solr/test_collection_shard1_replica1/home/kiko/tools/solr-6.6.3/server/solr/test_collection_shard2_replica1 配置目录是否被其他集合使用。如果没有，那么该目录将从SolrCloud 集群 中删除1[root@kiko1 solr]# /home/kiko/tools/solr-6.6.3/bin/solr delete -c test_collection 响应1234567891011121314151617181920Connecting to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181INFO - 2018-05-10 09:58:23.083; org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider; Cluster at kiko1:2181,kiko2:2181,kiko3:2181 readyDeleting collection &apos;test_collection&apos; using command:http://kiko2:8983/solr/admin/collections?action=DELETE&amp;name=test_collection&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:825&#125;, &quot;success&quot;:&#123; &quot;kiko3:8983_solr&quot;:&#123;&quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:40&#125;&#125;, &quot;kiko1:8983_solr&quot;:&#123;&quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:41&#125;&#125;, &quot;kiko2:8983_solr&quot;:&#123;&quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:45&#125;&#125;&#125;&#125; 停止集群在任意一台机器,停止SolrCloud集群,在SolrCloud模式下停止Solr，可以使用 -all1[root@kiko1 solr]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/solr-6.6.3/bin/solr stop -all&quot; ; done 或者1[root@kiko1 solr]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/solr-6.6.3/bin/solr stop -cloud -z kiko1:2181,kiko2:2181,kiko3:2181 -p 8983 -force&quot; ; done 响应123Sending stop command to Solr running on port 8983 ... waiting up to 180 seconds to allow Jetty process 10860 to stop gracefully.Sending stop command to Solr running on port 8983 ... waiting up to 180 seconds to allow Jetty process 24326 to stop gracefully.Sending stop command to Solr running on port 8983 ... waiting up to 180 seconds to allow Jetty process 9689 to stop gracefully. 副本状态healthcheck 命收集有关集合中每个副本的基本信息，例如副本数量，当前运行状态，是否正常，以及每个副本运行多长时间，内存 和地址（副本在群集中的位置）1[root@kiko1 solr]# /home/kiko/tools/solr-6.6.3/bin/solr healthcheck -c test_collection -z kiko1:2181,kiko2:2181,kiko3:2181 响应1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859INFO - 2018-05-10 10:19:41.025; org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider; Cluster at kiko1:2181,kiko2:2181,kiko3:2181 ready&#123; &quot;collection&quot;:&quot;test_collection&quot;, &quot;status&quot;:&quot;healthy&quot;, &quot;numDocs&quot;:0, &quot;numShards&quot;:2, &quot;shards&quot;:[ &#123; &quot;shard&quot;:&quot;shard1&quot;, &quot;status&quot;:&quot;healthy&quot;, &quot;replicas&quot;:[ &#123; &quot;name&quot;:&quot;core_node1&quot;, &quot;url&quot;:&quot;http://kiko1:8983/solr/test_collection_shard1_replica2/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 46 seconds&quot;, &quot;memory&quot;:&quot;42.1 MB (%8.6) of 490.7 MB&quot;&#125;, &#123; &quot;name&quot;:&quot;core_node3&quot;, &quot;url&quot;:&quot;http://kiko3:8983/solr/test_collection_shard1_replica3/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 34 seconds&quot;, &quot;memory&quot;:&quot;30.1 MB (%6.1) of 490.7 MB&quot;, &quot;leader&quot;:true&#125;, &#123; &quot;name&quot;:&quot;core_node6&quot;, &quot;url&quot;:&quot;http://kiko2:8983/solr/test_collection_shard1_replica1/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 40 seconds&quot;, &quot;memory&quot;:&quot;104 MB (%21.2) of 490.7 MB&quot;&#125;]&#125;, &#123; &quot;shard&quot;:&quot;shard2&quot;, &quot;status&quot;:&quot;healthy&quot;, &quot;replicas&quot;:[ &#123; &quot;name&quot;:&quot;core_node2&quot;, &quot;url&quot;:&quot;http://kiko1:8983/solr/test_collection_shard2_replica2/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 46 seconds&quot;, &quot;memory&quot;:&quot;42.6 MB (%8.7) of 490.7 MB&quot;&#125;, &#123; &quot;name&quot;:&quot;core_node4&quot;, &quot;url&quot;:&quot;http://kiko3:8983/solr/test_collection_shard2_replica3/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 34 seconds&quot;, &quot;memory&quot;:&quot;31 MB (%6.3) of 490.7 MB&quot;, &quot;leader&quot;:true&#125;, &#123; &quot;name&quot;:&quot;core_node5&quot;, &quot;url&quot;:&quot;http://kiko2:8983/solr/test_collection_shard2_replica1/&quot;, &quot;numDocs&quot;:0, &quot;status&quot;:&quot;active&quot;, &quot;uptime&quot;:&quot;0 days, 0 hours, 9 minutes, 40 seconds&quot;, &quot;memory&quot;:&quot;27.6 MB (%5.6) of 490.7 MB&quot;&#125;]&#125;]&#125; ZK管理solr配置配置文件上传到ZooKeeper 集群可用参数（所有参数都是必需的） -n 在ZooKeeper中设置的配置名称，可以通过管理界面，点击菜单，Cloud 选中 Tree / configs 下查看，配置列表 -d 配置设置为上传的路径。路径需要有一个“conf”目录，依次包含solrconfig.xml等。最好可以提供绝对路径 -z Zookeeper IP 端口，多个zk用”,” 分隔SolrCloud是通过Zookeeper集群来保证配置文件的变更及时同步到各个节点上，所以，可以将配置文件上传到Zookeeper集群。 例如 我们可以在/home/kiko/tools/solr-6.6.3/server/solr/configsets/basic_configs/conf/路径中对db-data-config.xml,managed-schema,solrconfig.xml进行配置，包括配置中文分词，导入数据库数据等,然后对test_collection进行配置文件上传，命令如下： 1[root@kiko2 solr]# /home/kiko/tools/solr-6.6.3/bin/solr zk upconfig -z kiko1:2181,kiko2:2181,kiko3:2181 -n test_collection -d /home/kiko/tools/solr-6.6.3/server/solr/configsets/basic_configs/ 响应12Connecting to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181 ...Uploading /home/kiko/tools/solr-6.6.3/server/solr/configsets/basic_configs/conf for config test_collection to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181 删除上传到ZooKeeper 集群的solr 配置rm 删除 -r 递归删除1[root@kiko1 solr]# /home/kiko/tools/solr-6.6.3/bin/solr zk rm -r /configs/mynewconfig -z kiko1:2181,kiko2:2181,kiko3:2181 响应12Connecting to ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181 ...Removing Zookeeper node /configs/mynewconfig from ZooKeeper at kiko1:2181,kiko2:2181,kiko3:2181 recurse: true 本文参考 CentOs7.3 搭建 SolrCloud 集群服务]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>solr，java</tag>
        <tag>集群</tag>
        <tag>Cluster</tag>
        <tag>zookeeper</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux搭建 ZooKeeper-3.4.12 Cluster 集群服务]]></title>
    <url>%2F2018%2F05%2F09%2Fzookeeper-3-4-12%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[Zookeeper 概述zookeeper实际上是yahoo开发的，用于分布式中一致性处理的框架。最初其作为研发Hadoop时的副产品。由于分布式系统中一致性处理较为困难，其他的分布式系统没有必要 费劲重复造轮子，故随后的分布式系统中大量应用了zookeeper，以至于zookeeper成为了各种分布式系统的基础组件，其地位之重要，可想而知。著名的hadoop，kafka，dubbo 都是基于zookeeper而构建。 想理解zookeeper到底是做啥的，那首先得理解清楚，什么是一致性？ 所谓的一致性，实际上就是围绕着“看见”来的。谁能看见？能否看见？什么时候看见？举个例子：淘宝后台卖家，在后台上架一件大促的商品，通过服务器A提交到主数据库，假设刚提交后立马就有用户去通过应用服务器B去从数据库查询该商品，就会出现一个现象，卖家已经更新成功了，然而买家却看不到；而经过一段时间后，主数据库的数据同步到了从数据库，买家就能查到了。 假设卖家更新成功之后买家立马就能看到卖家的更新，则称为强一致性 如果卖家更新成功后买家不能看到卖家更新的内容，则称为弱一致性 而卖家更新成功后，买家经过一段时间最终能看到卖家的更新，则称为最终一致性 《一致性协议》 《ZooKeeper应用场景》 《分布式架构》 《分布式 ZooKeeper 系列》 环境 Linux版本：Oracle Linux Server release 6.5 ZooKeeper版本：zookeeper-3.4.12.tar.gz 虚拟机IP：192.168.229.115,192.168.229.116,192.168.229.117 集群主机名称：kiko1,kiko2,kiko3 集群主机用户：root 集群JDK版本：jdk1.8.0_121 集群主机之间设置免密登陆：设置方式请参考 linux ssh 免密登录 注意事项关闭防火墙 iptables防火墙关闭命令1$ service iptables stop # 关闭命令： firewall防火墙关闭命令1$ systemctl stop firewalld.service # 停止firewall ZooKeeper 安装下载ZooKeeper12$ cd /home/kiko/tools/$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.12/zookeeper-3.4.12.tar.gz 提取tar文件12$ tar -xvf zookeeper-3.4.12.tar.gz$ cd zookeeper-3.4.12 创建配置文件zoo.cfg为zookeeper的主要配置文件，因为Zookeeper是一个集群服务，集群的每个节点都需要这个配置文件。为了避免出差错，zoo.cfg这个配置文件里没有跟特定节点相关的配置，所以每个节点上的这个zoo.cfg都是一模一样的配置。这样就非常便于管理了，比如我们可以把这个文件提交到版本控制里管理起来。其实这给我们设计集群系统的时候也是个提示：集群系统一般有很多配置，应该尽量将通用的配置和特定每个服务的配置(比如服务标识)分离，这样通用的配置在不同服务之间copy就ok了 编辑zoo.cfg1[root@kiko1 ~]# vim /home/kiko/tools/zookeeper-3.4.12/conf/zoo.cfg 编辑内容如下123456789dataDir = /home/kiko/tools/zookeeper-3.4.12/datadataLogDir = /home/kiko/tools/zookeeper-3.4.12/logstickTime = 2000clientPort = 2181initLimit = 5syncLimit = 2server.1=kiko1:2888:3888server.2=kiko2:2888:3888server.3=kiko3:2888:3888 配置文件描述 tickTime tickTime则是上述两个超时配置的基本单位，例如对于initLimit，其配置值为5，说明其超时时间为 2000ms * 5 = 10秒。 dataDir 其配置的含义跟单机模式下的含义类似，不同的是集群模式下还有一个myid文件。myid文件的内容只有一行，且内容只能为1 - 255之间的数字，这个数字亦即上面介绍server.id中的id，表示zk进程的id。 dataLogDir 如果没提供的话使用的则是dataDir。zookeeper的持久化都存储在这两个目录里。dataLogDir里是放到的顺序日志(WAL)。而dataDir里放的是内存数据结构的snapshot，便于快速恢复。为了达到性能最大化，一般建议把dataDir和dataLogDir分到不同的磁盘上，这样就可以充分利用磁盘顺序写的特性。 initLimit ZooKeeper集群模式下包含多个zk进程，其中一个进程为leader，余下的进程为follower。当follower最初与leader建立连接时，它们之间会传输相当多的数据，尤其是follower的数据落后leader很多。initLimit配置follower与leader之间建立连接后进行同步的最长时间。 syncLimit 配置follower和leader之间发送消息，请求和应答的最大时间长度。 server.id=host:port1:port2 server.id 其中id为一个数字，表示zk进程的id，这个id也是data目录下myid文件的内容 host 是该zk进程所在的IP地址 port1 表示follower和leader交换消息所使用的端口 port2 表示选举leader所使用的端口创建myid 文件在data里会放置一个myid文件，里面就一个数字，用来唯一标识这个服务。这个id是很重要的，一定要保证整个集群中唯一 ZooKeeper会根据这个id来取出server.x上的配置。比如当前id为1，则对应着zoo.cfg里的server.1的配置1[root@kiko1 ~]# echo &quot;1&quot;&gt;/home/kiko/tools/zookeeper-3.4.12/data/myid 这样一台kiko1机器就配置完了 复制集群配置在集群kiko1上执行,循环复制配置好的zookeeper到其他两台主机kiko2,kiko3上1[root@kiko1 data]# for a in &#123;2..3&#125; ; do scp -r /home/kiko/tools/zookeeper-3.4.12/ kiko$a:/home/kiko/tools/zookeeper-3.4.12 ; done 在集群kiko1 上执行 ,批量修改myid 文件1[root@kiko1 data]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; echo $a &gt; /home/kiko/tools/zookeeper-3.4.12/data/myid&quot; ; done 集群操作在集群任意一台机器上执行 启动集群1[root@kiko1 data]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/zookeeper-3.4.12/bin/zkServer.sh start&quot; ; done 响应123456789ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTEDZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 连接集群1[root@kiko1 /]# ./home/kiko/tools/zookeeper-3.4.12/bin/zkCli.sh -server kiko1:2181,kiko2:2181,kiko3:2181 响应1234567891011121314151617181920212223242526Connecting to kiko1:2181,kiko2:2181,kiko3:21812018-05-09 15:51:39,711 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMT2018-05-09 15:51:39,715 [myid:] - INFO [main:Environment@100] - Client environment:host.name=acct.dev.gyist.com2018-05-09 15:51:39,715 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1212018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/java/jdk1.8.0_121/jre2018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/home/kiko/tools/zookeeper-3.4.12/bin/../build/classes:/home/kiko/tools/zookeeper-3.4.12/bin/../build/lib/*.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/slf4j-log4j12-1.7.25.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/slf4j-api-1.7.25.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/netty-3.10.6.Final.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/log4j-1.2.17.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/jline-0.9.94.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/audience-annotations-0.5.0.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../zookeeper-3.4.12.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../src/java/lib/*.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../conf:.:/usr/java/jdk1.8.0_121/lib/dt.jar:/usr/java/jdk1.8.0_121/lib/tools.jar2018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/local/apr/lib:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2018-05-09 15:51:39,718 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:os.version=3.8.13-26.1.1.el6uek.x86_642018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2018-05-09 15:51:39,719 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/2018-05-09 15:51:39,721 [myid:] - INFO [main:ZooKeeper@441] - Initiating client connection, connectString=kiko1:2181,kiko2:2181,kiko3:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@67424e82Welcome to ZooKeeper!2018-05-09 15:51:39,747 [myid:] - INFO [main-SendThread(kiko3:2181):ClientCnxn$SendThread@1028] - Opening socket connection to server kiko3/192.168.229.117:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2018-05-09 15:51:39,842 [myid:] - INFO [main-SendThread(kiko3:2181):ClientCnxn$SendThread@878] - Socket connection established to kiko3/192.168.229.117:2181, initiating session[zk: kiko1:2181,kiko2:2181,kiko3:2181(CONNECTING) 0] 2018-05-09 15:51:39,893 [myid:] - INFO [main-SendThread(kiko3:2181):ClientCnxn$SendThread@1302] - Session establishment complete on server kiko3/192.168.229.117:2181, sessionid = 0x300044d1c310000, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null 从日志可以看出客户端成功连接的是kiko3 连接上哪台机器的zk进程是随机的123Welcome to ZooKeeper!2018-05-09 15:51:39,747 [myid:] - INFO [main-SendThread(kiko3:2181):ClientCnxn$SendThread@1028] - Opening socket connection to server kiko3/192.168.229.117:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled 集群状态1[root@kiko1 /]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/zookeeper-3.4.12/bin/zkServer.sh status&quot; ; done 响应123456789ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: followerZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: leaderZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: follower 通过日志我可以看到 kiko2 leader (ps 是老大)，其他 kiko1 ,kiko3 follower (ps 都是小弟) Leader 怎么选举的可以参考《Zookeeper的Leader选举》 停止集群1[root@kiko1 /]# for a in &#123;1..3&#125; ; do ssh kiko$a &quot;source /etc/profile; /home/kiko/tools/zookeeper-3.4.12/bin/zkServer.sh stop&quot; ; done 响应123456789ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStopping zookeeper ... STOPPEDZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStopping zookeeper ... STOPPEDZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED 本文参考 CentOs7.3 搭建 ZooKeeper-3.4.9 Cluster 集群服务]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Cluster</tag>
        <tag>zookeeper</tag>
        <tag>集群服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 搭建 ZooKeeper-3.4.12 单机服务]]></title>
    <url>%2F2018%2F05%2F09%2Fzookeeper-3-4-12%2F</url>
    <content type="text"><![CDATA[Zookeeper 概述zookeeper实际上是yahoo开发的，用于分布式中一致性处理的框架。最初其作为研发Hadoop时的副产品。由于分布式系统中一致性处理较为困难，其他的分布式系统没有必要 费劲重复造轮子，故随后的分布式系统中大量应用了zookeeper，以至于zookeeper成为了各种分布式系统的基础组件，其地位之重要，可想而知。著名的hadoop，kafka，dubbo 都是基于zookeeper而构建。 想理解zookeeper到底是做啥的，那首先得理解清楚，什么是一致性？ 所谓的一致性，实际上就是围绕着“看见”来的。谁能看见？能否看见？什么时候看见？举个例子：淘宝后台卖家，在后台上架一件大促的商品，通过服务器A提交到主数据库，假设刚提交后立马就有用户去通过应用服务器B去从数据库查询该商品，就会出现一个现象，卖家已经更新成功了，然而买家却看不到；而经过一段时间后，主数据库的数据同步到了从数据库，买家就能查到了。 假设卖家更新成功之后买家立马就能看到卖家的更新，则称为强一致性 如果卖家更新成功后买家不能看到卖家更新的内容，则称为弱一致性 而卖家更新成功后，买家经过一段时间最终能看到卖家的更新，则称为最终一致性 《一致性协议》 《ZooKeeper应用场景》 《分布式架构》 《分布式 ZooKeeper 系列》 环境 Linux版本：Oracle Linux Server release 6.5 ZooKeeper版本：zookeeper-3.4.12.tar.gz jdk版本：jdk1.8.0_121 注意事项关闭防火墙，根据Linux系统安装的防火墙选择相应的命令关闭 iptables1$ service iptables stop # 关闭命令： 关闭firewall1$ systemctl stop firewalld.service # 停止firewall ZooKeeper 安装下载ZooKeeper12$ cd /home/kiko/tools/$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.12/zookeeper-3.4.12.tar.gz 提取tar文件12$ tar -xvf zookeeper-3.4.12.tar.gz$ cd zookeeper-3.4.12 创建data文件夹 用于存储数据文件1$ mkdir /home/kiko/tools/zookeeper-3.4.12/data 创建logs文件夹 用于存储日志1$ mkdir /home/kiko/tools/zookeeper-3.4.12/logs 创建配置文件使用命令 vim conf/zoo.cfg 创建配置文件并打开，ps (其实目录conf 下有默认的配置文件，但是注释太多，英文一大堆，太乱) 1$ vim /home/kiko/tools/zookeeper-3.4.12/conf/zoo.cfg 编辑内容如下1234567tickTime = 2000dataDir = /home/kiko/tools/zookeeper-3.4.12/datadataLogDir = home/kiko/tools/zookeeper-3.4.12/logstickTime = 2000clientPort = 2181initLimit = 5syncLimit = 2 配置文件描述 tickTime 则是上述两个超时配置的基本单位，例如对于initLimit，其配置值为5，说明其超时时间为 2000ms * 5 = 10秒。 dataDir 其配置的含义跟单机模式下的含义类似，不同的是集群模式下还有一个myid文件。myid文件的内容只有一行，且内容只能为1 - 255之间的数字，这个数字亦即上面介绍server.id中的id，表示zk进程的id。 dataLogDir 如果没提供的话使用的则是dataDir。zookeeper的持久化都存储在这两个目录里。dataLogDir里是放到的顺序日志(WAL)。而dataDir里放的是内存数据结构的snapshot，便于快速恢复。为了达到性能最大化，一般建议把dataDir和dataLogDir分到不同的磁盘上，这样就可以充分利用磁盘顺序写的特性。 initLimit ZooKeeper集群模式下包含多个zk进程，其中一个进程为leader，余下的进程为follower。 当follower最初与leader建立连接时，它们之间会传输相当多的数据，尤其是follower的数据落后leader很多。initLimit配置follower与leader之间建立连接后进行同步的最长时间。 syncLimit 配置follower和leader之间发送消息，请求和应答的最大时间长度。 ZooKeeper操作启动服务1[kiko@kiko1 bin]$ ./zkServer.sh start 相应123ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 连接服务1[kiko@kiko1 bin]$ ./zkCli.sh 响应123456789101112131415161718192021222324252627Connecting to localhost:21812018-05-09 14:17:48,978 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMT2018-05-09 14:17:48,983 [myid:] - INFO [main:Environment@100] - Client environment:host.name=acct.dev.gyist.com2018-05-09 14:17:48,983 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1212018-05-09 14:17:48,986 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2018-05-09 14:17:48,986 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/java/jdk1.8.0_121/jre2018-05-09 14:17:48,986 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/home/kiko/tools/zookeeper-3.4.12/bin/../build/classes:/home/kiko/tools/zookeeper-3.4.12/bin/../build/lib/*.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/slf4j-log4j12-1.7.25.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/slf4j-api-1.7.25.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/netty-3.10.6.Final.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/log4j-1.2.17.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/jline-0.9.94.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../lib/audience-annotations-0.5.0.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../zookeeper-3.4.12.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../src/java/lib/*.jar:/home/kiko/tools/zookeeper-3.4.12/bin/../conf:.:/usr/java/jdk1.8.0_121/lib/dt.jar:/usr/java/jdk1.8.0_121/lib/tools.jar2018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:os.version=3.8.13-26.1.1.el6uek.x86_642018-05-09 14:17:48,987 [myid:] - INFO [main:Environment@100] - Client environment:user.name=kiko2018-05-09 14:17:48,988 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/home/kiko2018-05-09 14:17:48,988 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/home/kiko/tools/zookeeper-3.4.12/bin2018-05-09 14:17:48,989 [myid:] - INFO [main:ZooKeeper@441] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@67424e82Welcome to ZooKeeper!2018-05-09 14:17:49,016 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1028] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2018-05-09 14:17:49,107 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@878] - Socket connection established to localhost/127.0.0.1:2181, initiating session2018-05-09 14:17:49,119 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1302] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10003f83e3b0001, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] 服务状态1[kiko@kiko1 bin]$ ./zkServer.sh status 响应123ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: standalone 停止服务1[kiko@kiko1 bin]$ ./zkServer.sh stop 响应123ZooKeeper JMX enabled by defaultUsing config: /home/kiko/tools/zookeeper-3.4.12/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED 本文参考 CentOs7.3 搭建 ZooKeeper-3.4.9 单机服务]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>linux</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux ssh 免密登录]]></title>
    <url>%2F2018%2F05%2F08%2Flinux-ssh-free-passwd-login%2F</url>
    <content type="text"><![CDATA[环境 Linux版本 Oracle Linux Server release 6.5 三台虚拟机 192.168.229.115，192.168.229.116，192.168.229.117 修改主机名修改三台主机名，以此类推，kiko1，kiko2，kiko31# vim /etc/sysconfig/network 依次修改各机器的hostname12NETWORKING=yesHOSTNAME=kiko1 修改完成后重启相应机器使主机名生效1#reboot 修改映射关系 在kiko1的/etc/hosts文件添加如下内容 123192.168.229.115 kiko1192.168.229.116 kiko2192.168.229.117 kiko3 查看修改后的/etc/hosts文件内容 123456[root@kiko1 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.229.115 kiko1192.168.229.116 kiko2192.168.229.117 kiko3 将集群kiko1 上的文件hosts文件 通过 scp 命令复制发送到集群的每一个节点 1# for a in &#123;1..3&#125; ; do scp /etc/hosts kiko$a:/etc/hosts ; done 检查是否集群每一个节点的 hosts 文件都已经修改过来 1# for a in &#123;1..3&#125; ; do ssh kiko$a cat /etc/hosts ; done 启动ssh免密登录 在集群kiko1的 /etc/ssh/sshd_config 文件去掉以下选项的注释并保存 123# vim /etc/ssh/sshd_configRSAAuthentication yesPubkeyAuthentication yes 将集群kiko1 修改后的 /etc/ssh/sshd_config 通过 scp 命令复制发送到集群的每一个节点 1# for a in &#123;1..3&#125; ; do scp /etc/ssh/sshd_config kiko$a:/etc/ssh/sshd_config ; done 生成公钥、私钥 在集群的每一个节点节点输入命令 ssh-keygen -t rsa -P &#39;&#39;，生成 key，一律回车 12345678910111213141516171819[root@kiko1 ~]# ssh-keygen -t rsa -P &apos;&apos;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:ef:59:fa:c6:fa:da:b1:d3:ba:88:60:00:4a:35:c0:6f root@kiko1The key&apos;s randomart image is:+--[ RSA 2048]----+|...o || .. . || .o ||.. E ||. . . S || . . || o ..o. || . . o *+o. || . B*B+ |+-----------------+ 在集群的kiko1 节点输入命令 将集群每一个节点的公钥id_rsa.pub放入到自己的认证文件中authorized_keys;1for a in &#123;1..3&#125;; do ssh kiko$a cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys; done 在集群的kiko1 节点输入命令 将自己的认证文件 authorized_keys 通过 scp 命令复制发送到每一个节点上去: /root/.ssh/authorized_keys1for a in &#123;1..3&#125;; do scp /root/.ssh/authorized_keys kiko$a:/root/.ssh/authorized_keys ; done 在集群的每一个节点节点输入命令 接重启ssh服务1# service sshd restart 验证 ssh 免密登录 例如在kiko3机器 ssh登录kiko11234567[root@kiko3 ~]# ssh kiko1The authenticity of host &apos;kiko1 (192.168.229.115)&apos; can&apos;t be established.RSA key fingerprint is d9:33:04:3b:3a:d4:c3:1e:9e:bb:f1:bd:d9:bf:2a:3f.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;kiko1,192.168.229.115&apos; (RSA) to the list of known hosts.Last login: Tue May 8 20:29:25 from kiko2[root@kiko1 ~]# 表示登录成功 exit退出123[root@kiko1 ~]# exitlogoutConnection to kiko1 closed. 注意事项在重启sshd服务时,只能选择service sshd restart命令重启，切记千万不要对sshd服务进程进行kill操作,再进行启动，因为当你kill掉sshd服务进程后，你远程登录的机器将会自动退出并永远无法登录。特别是linux服务器不受自己控制或在异地的时候，惨痛的教训 本文参考：CentOs7.3 ssh 免密登录]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>linux</tag>
        <tag>ssh</tag>
        <tag>集群服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 搭建solr单机服务]]></title>
    <url>%2F2018%2F04%2F27%2F%E6%90%AD%E5%BB%BAsolr%E5%8D%95%E6%9C%BA%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[solr简介Solr是一款优秀的基于Lucene的全文检索服务器，它对Lucene进行了扩展，提供了非常丰富的查询语言，并对查询进行了性能优化。 Solr和Lucene都由Apache Software Foundation(www.apache.org)管理。 Apache Solr 参考指南 环境准备Linux版本：Linux version 3.8.13-26.1.1.el6uek.x86_64 (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) )Solr版本：solr-6.6.3Jdk环境：1.8.0_161 注意事项关闭防火墙1$ service iptables stop Solr安装下载Solr下载最新版本的Solr1$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/6.6.3/solr-6.6.3.zip 提取zip文件12$ unzip -xvf solr-6.6.3.zip$ cd solr-6.6.3 Solr操作启动服务 说明：加 -force 是因为solr不允许使用 root 进行操作的,其他账户可不加 1$ bin/solr start -force 响应12345Warning: Available entropy is low. As a result, use of the UUIDField, SSL, or any other features that requireRNG might not work properly. To check for the amount of available entropy, use &apos;cat /proc/sys/kernel/random/entropy_avail&apos;.Waiting up to 180 seconds to see Solr running on port 8983 [\]Started Solr server on port 8983 (pid=12419). Happy searching! 启动Solr与不同的端口，要更改Solr监听端口，可以-p在启动时使用参数1$ bin/solr start -p 8984 访问管理UI浏览器输入 ip:port 即：http://192.168.229.119:8983/solr 如果Solr没有运行，您的浏览器会抱怨说它无法连接到服务器。检查您的端口号，然后重试。 服务状态如果您不确定Solr是否在本地运行1$ bin/solr status 响应123456789Found 1 Solr nodes:Solr process 12419 running on port 8983&#123; &quot;solr_home&quot;:&quot;/home/kiko/tools/solr-6.6.3/server/solr&quot;, &quot;version&quot;:&quot;6.6.3 d1e9bbd333ea55cfa0c75d324424606e857a775b - sarowe - 2018-03-02 15:09:34&quot;, &quot;startTime&quot;:&quot;2018-04-27T04:25:39.582Z&quot;, &quot;uptime&quot;:&quot;0 days, 1 hours, 46 minutes, 35 seconds&quot;, &quot;memory&quot;:&quot;33.6 MB (%6.8) of 490.7 MB&quot;&#125; 表示Solr服务启动正常 创建Solr库1$ bin/solr create -c &lt;name&gt; 说明：加 -force 是因为solr不允许使用 root 进行操作的,其他账户可不加 1$ /home/kiko/tools/solr-6.6.3/bin/solr create -c test -force 响应1234567891011Copying configuration to new core instance directory:/home/kiko/tools/solr-6.6.3/server/solr/testCreating new core &apos;test&apos; using command:http://localhost:8983/solr/admin/cores?action=CREATE&amp;name=test&amp;instanceDir=test&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:2146&#125;, &quot;core&quot;:&quot;test&quot;&#125; 浏览器输入 ip:port http://192.168.229.119:8983/solr 停止服务1$ /home/kiko/tools/solr-6.6.3/bin/solr stop 响应1Sending stop command to Solr running on port 8983 ... waiting up to 180 seconds to allow Jetty process 12419 to stop gracefully. 可以使用该-all参数来停止所有运行的Solr实例1$ /home/kiko/tools/solr-6.6.3/bin/solr stop -all 本文参考：http://www.ymq.io/2017/08/23/Solr/]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>solr，java</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL创建用户名密码数据库及授权]]></title>
    <url>%2F2018%2F04%2F24%2Fmysql-create-usernameAndPasswordAndDatabase%2F</url>
    <content type="text"><![CDATA[管理员权限登录首先使用管理员权限登录到数据库123456789101112131415[root@kiko2 ~]# mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 227206Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 创建数据库通过管理员权限创建数据库设置字符集12-- 创建数据库并设置字符集mysql&gt; CREATE DATABASE 数据库名 DEFAULT CHARACTER SET utf8; 创建远程访问用户名密码并授权数据库权限创建远程访问用户名密码并授权数据库操作权限12-- 创建远程访问用户名密码并授权数据库操作权限mysql&gt; GRANT CREATE,DROP,GRANT OPTION,LOCK TABLES,REFERENCES,EVENT,ALTER,DELETE,INDEX,INSERT,SELECT,UPDATE,CREATE TEMPORARY TABLES,TRIGGER,CREATE VIEW,SHOW VIEW,ALTER ROUTINE,CREATE ROUTINE,EXECUTE ON 数据库名.* TO 用户名@"%" IDENTIFIED BY '密码'; 创建本地访问用户名密码并授权数据库操作权限创建本地访问用户名密码并授权数据库操作权限12-- 创建本地访问用户名密码并授权数据库操作权限mysql&gt; GRANT CREATE,DROP,GRANT OPTION,LOCK TABLES,REFERENCES,EVENT,ALTER,DELETE,INDEX,INSERT,SELECT,UPDATE,CREATE TEMPORARY TABLES,TRIGGER,CREATE VIEW,SHOW VIEW,ALTER ROUTINE,CREATE ROUTINE,EXECUTE ON 数据库名.* TO 用户名@localhost IDENTIFIED BY '密码';]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
        <tag>数据库</tag>
        <tag>授权</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql主从复制]]></title>
    <url>%2F2018%2F04%2F16%2Fmysql-master-slave-sync%2F</url>
    <content type="text"><![CDATA[概念主从复制可以使MySQL数据库主服务器的主数据库，复制到一个或多个MySQL从服务器从数据库，默认情况下，复制异步; 根据配置，可以复制数据库中的所有数据库，选定的数据库或甚至选定的表。 MySQL中主从复制的优点 横向扩展解决方案 在多个从库之间扩展负载以提高性能。在这种环境中，所有写入和更新在主库上进行。但是，读取可能发生在一个或多个从库上。该模型可以提高写入的性能（由于主库专用于更新），同时在多个从库上读取，可以大大提高读取速度。 数据安全性 由于主库数据被复制到从库，从库可以暂停复制过程，可以在从库上运行备份服务，而不会破坏对应的主库数据。 分析 可以在主库上创建实时数据，而信息分析可以在从库上进行，而不会影响主服务器的性能。 长距离数据分发 可以使用复制创建远程站点使用的数据的本地副本，而无需永久访问主库。 环境准备 MySQL版本 MySQL 5.7.21 Master-Server 192.168.229.116 Slave-Server 192.168.229.119 安装MySQL参考上篇博文首先在两台机器上装上，保证正常启动，可以使用 Master-Server 配置修改 my.cnf配置 Master 以使用基于二进制日志文件位置的复制，必须启用二进制日志记录并建立唯一的服务器ID,否则则无法进行主从复制。 开启binlog ，每台设置不同的 server-id1234$ cat /etc/my.cnf[mysqld]log-bin=mysql-binserver-id=1 重启MySQL服务1$ service mysql restart 登录MySQL1$ mysql -u root -p 创建备份用户12mysql&gt; CREATE USER &apos;backup&apos;@&apos;192.168.229.119&apos; IDENTIFIED BY &apos;backup&apos;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;backup&apos;@&apos;192.168.252.119&apos;; 每个从库使用MySQL用户名和密码连接到主库，因此主库上必须有用户帐户，从库可以连接。任何帐户都可以用于此操作，只要它已被授予 REPLICATION SLAVE权限。可以选择为每个从库创建不同的帐户，或者每个从库使用相同帐户连接到主库 虽然不必专门为复制创建帐户，但应注意，复制用到的用户名和密码会以纯文本格式存储在主信息存储库文件或表中 。因此，需要创建一个单独的帐户，该帐户只具有复制过程的权限，以尽可能减少对其他帐户的危害。 Slave-Server 配置修改 my.cnf123$ cat /etc/my.cnf[mysqld]server-id=2 如果要设置多个从库，则每个从库的server-id与主库和其他从库设置不同的唯一值。 重启MySQL服务1$ service mysql restart 登录MySQL1$ mysql -u root -p 配置主库通信查看 Master-Server ， binlog File 文件名称和 Position值位置 并且记下来12345678910mysql&gt; show master status\G*************************** 1. row *************************** File: mysql-bin.000001 Position: 1697 Binlog_Do_DB: Binlog_Ignore_DB:Executed_Gtid_Set:1 row in set (0.00 sec)mysql&gt; 要设置从库与主库进行通信，进行复制，使用必要的连接信息配置从库在从库上执行以下语句将选项值替换为与系统相关的实际值 配置从库通信1CHANGE MASTER TO MASTER_HOST=&apos;192.168.229.116&apos;, MASTER_USER=&apos;backup&apos;, MASTER_PASSWORD=&apos;backup&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS=1697; 启动从服务器复制线程12mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec) 查看复制状态123456789101112131415mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.229.116 Master_User: slave Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 1697 Relay_Log_File: kiko-relay-bin.000005 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes..... 检查主从复制通信状态 Slave_IO_State： #从站的当前状态 Slave_IO_Running： Yes #读取主程序二进制日志的I/O线程是否正在运行 Slave_SQL_Running： Yes #执行读取主服务器中二进制日志事件的SQL线程是否正在运行。与I/O线程一样 Seconds_Behind_Master #是否为0，0就是已经同步了 必须都是 Yes 测试主从复制启动mysql服务并在主库中创建表1234567mysql&gt; create database demo;Query OK, 1 row affected (0.00 sec)mysql&gt; use demo;Database changedmysql&gt; create table users(id int not null auto_increment,name varchar(32) not null, primary key(id)) engine=InnoDB auto_increment=0 default charset=utf8;Query OK, 0 rows affected (0.03 sec) 从库中查询库表1234567891011mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || demo || mysql || performance_schema || sys |+--------------------+5 rows in set (0.01 sec) 主库中刚才创建的数据库demo已经同步到存库中,即标识一个主从复制的数据库配置已经大功告成 注意事项如果在用启动mysql&gt; start slave;启动从服务器复制线程是出错，可使用reset slave进行重置 参考资料： https://segmentfault.com/a/1190000010867488 http://www.ymq.io/2017/08/26/mysql-Master-Slave/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>环境配置</tag>
        <tag>mysql</tag>
        <tag>主从复制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下mysql安装]]></title>
    <url>%2F2018%2F04%2F13%2Fmysql-install-linux%2F</url>
    <content type="text"><![CDATA[准备工作下载mysql安装包，地址https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz 上传将本地下载好的安装包mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz上传到/usr/local目录下 解压123456#切换到安装目录下[root@kiko ~]# cd /usr/local/#解压文件[root@kiko local]# tar -zxvf mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz#重命名[root@kiko local]# mv mysql-5.7.21-linux-glibc2.12-x86_64 mysql 新建用户和用户组1234#新建用户[root@kiko local]# useradd mysql#新建用户组[root@kiko local]# groupadd mysql 初始化mysql数据库初始化，记录临时密码的值，在root@localhost:后面，如本人为：Wkj14!mSJsvS12345678[root@kiko local]# ./mysql/bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data2018-04-16T08:59:34.146011Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2018-04-16T08:59:35.362600Z 0 [Warning] InnoDB: New log files created, LSN=457902018-04-16T08:59:35.542848Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2018-04-16T08:59:35.613718Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 7c789a4b-4154-11e8-9f03-005056b460a8.2018-04-16T08:59:35.616751Z 0 [Warning] Gtid table is not ready to be used. Table &apos;mysql.gtid_executed&apos; cannot be opened.2018-04-16T08:59:35.617762Z 1 [Note] A temporary password is generated for root@localhost: Wkj14!mSJsvS[root@kiko local]# 将mysql启动服务添加只启动项1[root@kiko local]# cp ./mysql/support-files/mysql.server /etc/init.d/mysql 配置mysql环境变量打开环境变量配置文件1[root@kiko local]# vim /etc/profile 在/etc/profile文件中添加MYSQL_HOME配置，PATH变量追加$MYSQL_HOME/bin1234MYSQL_HOME=/usr/local/mysqlPATH=$JAVA_HOME/bin:$MYSQL_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH 使配置立即生效1[root@kiko local]# source /etc/profile 启动mysql服务12[root@kiko /]# service mysql startStarting MySQL. [ OK ] mysql服务启动 service mysql start mysql服务停止 service mysql stop mysql服务重启 service mysql restart mysql服务状态 service mysql status 测试登陆，输入临时密码 ，修改密码，退出后再次登陆测试12[root@kiko /]# mysql -u root -pEnter password: ##这里输入刚才生成的临时密码Wkj14!mSJsvS 出现以下提示代表成功12345678910111213Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 4Server version: 5.7.21Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 重置临时密码12mysql&gt; SET PASSWORD FOR &apos;root&apos;@&apos;localhost&apos; = PASSWORD(&apos;admin&apos;);Query OK, 0 rows affected, 1 warning (0.00 sec) 用新密码测试登录12[root@kiko /]# mysql -u root -pEnter password: ##这里 输入新密码admin 设置远程登录权限1234567891011##切换到mysql库mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed## root用户授权远程登录权限mysql&gt; grant all privileges on *.* to &apos;root&apos; @&apos;%&apos; identified by &apos;admin&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; 设置开机自启动1234# 添加到系统自启服务中chkconfig --add mysql# 设置开机自启chkconfig mysql on 安装中错误处理 在初次启动mysql服务时出现了报错现象如下12[root@kiko local]# service mysql startStarting MySQL.The server quit without updating PID file (/[FAILED]mysql/kiko.pid). rm -rf /etc/my.cnf删除掉这个文件后重启OK 卸载mysql，有时候在安装前linux服务器中曾经安装过或者自带有mysql安装文件，会导致安装错误，用以下命令对系统中的mysql文件进行搜索并强制删除1find / -name &quot;*mysql*&quot; | xargs rm -rf]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>mysql</tag>
        <tag>mysql安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下JDK环境变量配置]]></title>
    <url>%2F2018%2F04%2F12%2Fjdk-install-linux%2F</url>
    <content type="text"><![CDATA[准备工作 检查系统 12-[root@apply tool]# uname -a Linux localhost 3.8.13-26.1.1.el6uek.x86_64 #2 SMP Thu Feb 13 19:42:43 PST 2014 x86_64 x86_64 x86_64 GNU/Linux 下载jdk根据系统类型及位数下载相应的jdk，刚才查询结果本机属于64位linux系统，jdk下载地址http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 解压将下载的jdk解压到 /usr路径12[root@localhost ~]# cd /usr/[root@localhost usr]# tar -xvf /opt/tool/jdk-8u161-linux-x64.tar.gz 配置环境变量 编辑/etc/profile文件 1[root@localhost /]# vim etc/profile 在profile的最后加入/修改： 123JAVA_HOME=/usr/jdk1.8.0_161PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 更新/etc/profile文件让变量立即生效 1[root@localhost /]# source /etc/profile 验证输入java -version查看jdk版本，如果出现如下信息，标识配置成功1234[root@localhost /]# java -versionjava version &quot;1.8.0_161&quot;Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>java</tag>
        <tag>jdk安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git教程，将本地已有项目托管到码云]]></title>
    <url>%2F2018%2F03%2F28%2Fgit%2F</url>
    <content type="text"><![CDATA[准备工作 注册项目托管账号，国内有coding,gitee等，国外有github等，根据个人喜好。本文以gitee为例。 本地电脑安装git管理工具，网上教程有很多，如果是win7系统 可参考 这篇文章，如果是Mac或者Linux系统，一般都自带git。 添加ssh秘钥，参考 这篇文章。 码云上创建一个项目本地创建一个同名项目本地创建一个项目或文件夹 然后使用git bash初始化本地仓库使用 git init 命令 ，初始化一个git 本地仓库（项目）,会在本地创建一个 .git 的文件夹添加远程仓库使用git remote add origin https://gitee.com/你的码云用户名/kiko注意,我这里使用的是ssh方式，如果使用https方式每次提交都要输入远程仓库的用户名密码和密码git remote add origin git@gitee.com:leiyatao/kiko.git拉取远程文件到本地使用 git pull origin master 命令，将码云上的仓库pull到本地文件夹添加本地文件到缓存区使用git add . 或者 git add + filename (将文件保存到缓存区)提交文件到本地仓库使用git commit -m &#39;描述新添加的文件内容&#39; (就是注释) （文件保存到本地仓库）推送本地仓库内容到远程仓库使用git push origin master ，将本地仓库推送到远程仓库推送结果如下补充说明使用git config --global user.name &quot;Your Name Here&quot;命令修改提交作者]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>oschia</tag>
        <tag>gitee</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux top命令详解]]></title>
    <url>%2F2018%2F02%2F05%2Flinux-command-top%2F</url>
    <content type="text"><![CDATA[Linux命令工具 top详解top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。常在linux系统下玩，这是必须掌握的命令之一；下面详细说说这个命令： 执行top命令进入如下界面：12345678910top - 09:14:56 up 264 days, 20:56, 1 user, load average: 0.02, 0.04, 0.00Tasks: 87 total, 1 running, 86 sleeping, 0 stopped, 0 zombieCpu(s): 0.0%us, 0.2%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.2%stMem: 377672k total, 322332k used, 55340k free, 32592k buffersSwap: 397308k total, 67192k used, 330116k free, 71900k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 2856 656 388 S 0.0 0.2 0:49.40 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 7:15.20 ksoftirqd/0 4 root RT 0 0 0 0 S 0.0 0.0 0:00.00 migration/0 第一行1top - 16:47:09 up 129 days, 21:57, 3 users, load average: 0.00, 0.01, 0.05 09:14:56 ： 系统当前时间264 days, 20:56 ： 系统开机到现在经过了多少时间1 users ： 当前2用户在线load average: 0.02, 0.04, 0.00： 系统1分钟、5分钟、15分钟的CPU负载信息 第二行1Tasks: 87 total, 1 running, 86 sleeping, 0 stopped, 0 zombie Tasks：任务;87 total：很好理解，就是当前有87个任务，也就是87个进程。1 running：1个进程正在运行86 sleeping：86个进程睡眠0 stopped：停止的进程数0 zombie：僵死的进程数 第三行1Cpu(s): 0.0%us, 0.2%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.2%st Cpu(s)：表示这一行显示CPU总体信息0.0%us：用户态进程占用CPU时间百分比，不包含renice值为负的任务占用的CPU的时间。0.7%sy：内核占用CPU时间百分比0.0%ni：改变过优先级的进程占用CPU的百分比99.3%id：空闲CPU时间百分比0.0%wa：等待I/O的CPU时间百分比0.0%hi：CPU硬中断时间百分比0.0%si：CPU软中断时间百分比注：这里显示数据是所有cpu的平均值，如果想看每一个cpu的处理情况，按1即可；折叠，再次按1； 第四行1Mem: 377672k total, 322332k used, 55340k free, 32592k buffers Men：内存的意思377672k total：物理内存总量322332k used：使用的物理内存量55340k free：空闲的物理内存量32592k buffers：用作内核缓存的物理内存量 第五行1Swap: 397308k total, 67192k used, 330116k free, 71900k cached Swap：交换空间6881272k total：交换区总量4010444k used：使用的交换区量2870828k free：空闲的交换区量4336992k cached：缓冲交换区总量 进程信息1PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 再下面就是进程信息：PID：进程的IDUSER：进程所有者PR：进程的优先级别，越小越优先被执行NInice：值VIRT：进程占用的虚拟内存RES：进程占用的物理内存SHR：进程使用的共享内存S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数%CPU：进程占用CPU的使用率%MEM：进程使用的物理内存和总内存的百分比TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。COMMAND：进程启动命令名称 top命令交互操作指令下面列出一些常用的 top命令操作指令12345678910111213141516171819q：退出top命令&lt;Space&gt;：立即刷新s：设置刷新时间间隔c：显示命令完全模式t:：显示或隐藏进程和CPU状态信息m：显示或隐藏内存状态信息l：显示或隐藏uptime信息f：增加或减少进程显示标志S：累计模式，会把已完成或退出的子进程占用的CPU时间累计到父进程的MITE+P：按%CPU使用率排行T：按MITE+排行M：按%MEM排行u：指定显示用户进程r：修改进程renice值kkill：进程i：只显示正在运行的进程W：保存对top的设置到文件~/.toprc，下次启动将自动调用toprc文件的设置。h：帮助命令。q：退出]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中tomcat启动报错]]></title>
    <url>%2F2018%2F01%2F30%2Ftomcat-1%2F</url>
    <content type="text"><![CDATA[今天在linux同一服务器部署部署两个tomcat，tomcat1启动正常，tomcat2启动时包如下错误：123456ERROR: transport error 202: bind failed: Address already in useERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [debugInit.c:750]ERROR: transport error 202: bind failed: Address already in useERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [debugInit.c:750] 通过分析第一行错误，肯定是由于端口占用问题造成，但我在启动tomcat前已经对server.xml中的端口进行了更改，保证两个tomcat服务端口不冲突。后来在网上查找原因说是由于tomcat开通了远程调试端口，随即想起之前通过idea对服务器上的tomcat进行过远程调试，修改过tomcat中bin目录下的catalina.sh文件，123456789# -----------------------------------------------------------------------------# 支持Intellij IDEA 远程Debugexport JAVA_OPTS=&apos;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005&apos;# OS specific support. $var _must_ be set to either true or false.cygwin=falsedarwin=falseos400=false 从上文可看出，由于两个tomcat中的远程调试端口都为5005所以造成端口冲突，解决办法，注释远程调试配置或者将两个tomcat中的远程调试端口设置不一样即可。]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tomcat</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 常用命令(不断更新)]]></title>
    <url>%2F2017%2F12%2F09%2Flinux%2F</url>
    <content type="text"><![CDATA[linux常用命令整理 工作中常用linux命令整理 检查占用端口命令1234-- 检查所有占用端口netstat -ntlp-- 查询指定端口被占用 例如8080netstat -apn | grep 8080 检查占用端口命令1234-- 检查所有占用端口netstat -ntlp-- 查询指定端口被占用 例如8080netstat -apn | grep 8080 常用指令123456789101112131415161718192021222324ls 显示文件或目录 -l 列出文件详细信息l(list) -a 列出当前目录下所有文件及目录，包括隐藏的a(all)mkdir 创建目录 -p 创建目录，若无父目录，则创建p(parent)cd 切换目录touch 创建空文件echo 创建带有内容的文件。cat 查看文件内容cp 拷贝mv 移动或重命名rm 删除文件 -r 递归删除，可删除子目录及文件 -f 强制删除find 在文件系统中搜索某文件wc 统计文本中行数、字数、字符数grep 在文本文件中查找某个字符串rmdir 删除空目录tree 树形结构显示目录，需要安装tree包pwd 显示当前目录ln 创建链接文件more、less 分页显示文本文件内容head、tail 显示文件头、尾内容ctrl+alt+F1 命令行全屏模式 检查服务是否启动1234-- 检查tomcat服务是否启动ps -ef|grep tomcat-- 检查nginx服务是否启动ps -ef|grep nginx vim命令 vim三种模式，分别为：命令模式、插入模式、编辑模式。使用Esc或者i按键来切换模式。Esc切换到命令模式，i切换到编辑模式。12345678910命令模式下：:q 退出:q! 强制退出:wq 保存并退出:set number 显示行号:set nonumber 隐藏行号/apache 在文档中查找apache 按n跳到下一个，shift+n上一个yyp 复制光标所在行，并粘贴h(左移一个字符←)、j(下一行↓)、k(上一行↑)、l(右移一个字符→) 用户及用户组管理1234567891011121314151617/etc/passwd 存储用户账号/etc/group 存储组账号/etc/shadow 存储用户账号的密码/etc/gshadow 存储用户组账号的密码useradd 添加用户名userdel 删除用户名adduser 添加用户名groupadd 添加组名groupdel 删除组名passwd root 给root设置密码su rootsu – root/etc/profile 系统环境变量bash_profile 用户环境变量.bashrc 用户环境变量su user 切换用户，加载配置文件.bashrcsu – user 切换用户，加载配置文件/etc/profile ，加载bash_profile 关机/重启命令123456shutdown -r 关机重启 -h 关机不重启 now 立刻关机halt 关机reboot 重启 打包压缩相关命令123456789gzip：bzip2：tar: 打包压缩 -c 归档文件 -x 压缩文件 -z gzip压缩文件 -j bzip2压缩文件 -v 显示压缩或解压缩过程 v(view) -f 使用档名 举例：123tar -cvf /home/abc.tar /home/abc 只打包，不压缩tar -zcvf /home/abc.tar.gz /home/abc 打包，并用gzip压缩tar -jcvf /home/abc.tar.bz2 /home/abc 打包，并用bzip2压缩 解压当然，如果想解压缩，就直接替换上面的命令tar -cvf / tar -zcvf / tar -jcvf 中的“c” 换成“x” 就可以了123tar -xvf /home/abc.tar /home/abc.tar 只打包，不压缩tar -zxvf /home/abc.tar.gz /home/abc.zip 打包，并用gzip压缩tar -jxvf /home/abc.tar.bz2 /home/abc.zip 打包，并用bzip2压缩 系统管理命令12345678910111213141516stat 显示指定文件的详细信息，比ls更详细who 显示在线登陆用户whoami 显示当前操作用户hostname 显示主机名uname 显示系统信息top 动态显示当前耗费资源最多进程信息ps 显示瞬间进程状态 ps -auxdu 查看目录大小 du -h /home带有单位显示目录信息df 查看磁盘大小 df -h 带有单位显示磁盘信息ifconfig 查看网络情况ping 测试网络连通netstat 显示网络状态信息man 命令不会用了，找男人? 如：man lsclear 清屏alias 对命令重命名 如：alias showmeit=”ps -aux” ，另外解除使用unaliax showmeitkill 杀死进程，可以先用ps 或 top命令查看进程的id，然后再用kill命令杀死进程。 防火墙设置firewalld防火墙1234567891011121314151617181920212223242526272829303132333435363738391、firewalld的基本使用启动： systemctl start firewalld查看状态： systemctl status firewalld停止： systemctl disable firewalld禁用： systemctl stop firewalld2.systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed3.配置firewalld-cmd查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic那怎么开启一个端口呢添加firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效）重新载入firewall-cmd --reload查看firewall-cmd --zone= public --query-port=80/tcp删除firewall-cmd --zone= public --remove-port=80/tcp --permanent]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.2上tomcat启动慢问题解决]]></title>
    <url>%2F2017%2F11%2F30%2Ftomcat%2F</url>
    <content type="text"><![CDATA[今天在阿里云CentOS 7.2 启动Tomcat，发现启动很慢，需要几分钟，并且是在没有部署war包的情况下，这个问题值得重视，所以就去查看日志，发现耗时是session引起随机数问题导致的。Tomcat的Session ID通过SHA1算法计算得到的，计算Session ID的时候必须有1个秘钥，为了提高安全性Tomcat在启动的时候通过随机数生成秘钥。 一、环境介绍系统版本：CentOS 7.2软件版本：Tomcat 8 二、日志分析,排查原因日志如下：1230-Nov-2017 19:46:47.436 WARNING [localhost-startStop-1] org.apache.catalina.util.SessionIdGeneratorBase.createSecureRandom Creation of SecureRandom instance for session ID generation using [SHA1PRNG] took [182,556] milliseconds.30-Nov-2017 19:46:47.458 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /home/apache-tomcat-8.0.47/webapps/manager has finished in 183,080 ms 主要原因：就是通过随机数生成秘钥的时候卡住了，导致Tomcat启动慢或失败。 #影响随机数的强度的是生成用的熵，具体含义可以自己度娘 查下，不在这里细说了。查看是否有足够的熵来用于产生随机数，可以通过如下命令来查看12[root@qiuyuetao tools]# cat/proc/sys/kernel/random/entropy_avail 7 为了加速/dev/random提供随机数的速度，你可以通过操作设备的外设，让其产生大量的中断（如网络传输数据，按键，移动鼠标，在命令行敲几个不同的命令，俗称聚气。cat /dev/random ##可以消耗能量 三、处理及优化解决方法有3种； 方法1：使用rngd 软件增大熵池 *建议使用grep rdrand /proc/cpuinfo #需要cpu支持yum install rng-tools # 安装rngd服务（熵服务，增大熵池）systemctl start rngd # 启动服务 方法2：java环境下修改配置文件vim $JAVA_HOME/jre/lib/security/java.securitysecurerandom.source=file:/dev/random改为securerandom.source=file:/dev/urandom 方法3：可以通过配置JRE使用非阻塞的Entropy Source：vim $TOMCAT_HOME/bin/catalina.sh 文件中 JAVA_OPTS=”$JAVA_OPTS $JSSE_OPTS” 下添加如下shell1234567891011 JAVA_OPTS=&quot;$JAVA_OPTS $JSSE_OPTS&quot;################################################################if [[ &quot;$JAVA_OPTS&quot; != *-Djava.security.egd=* ]]; then JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.egd=file:/dev/urandom&quot;fi################################################################ 经过验证发现第三种方法最为靠谱优化前130-Nov-2017 19:55:11.656 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 318445 ms 优化后：130-Nov-2017 20:29:53.580 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 1335 ms]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tomcat</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次阿里云服务系统奔溃事件及处理过程]]></title>
    <url>%2F2017%2F10%2F13%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F%E5%A5%94%E6%BA%83%E4%BA%8B%E4%BB%B6%E5%8F%8A%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[昨晚阿里云上部署的应用服务突然无法访问，以为是服务挂了，准备ssh远程上去看个究竟。以前也出现过类似的情况，解决方案是远程上去后重启tomcat服务马上就恢复正常。这次准备ssh竟然连接不上，瞬间眼前一黑。打电话问同事是否做了什么操作，同事反馈，由于服务无法访问，他直接把系统执行reboot给重启了。reboot后直接无法ssh连接，也就是系统不能正常重启。登录阿里云控制台重启ECS实例依旧不成功，通过阿里云控制台实例的远程连接 进入管理终端，界面显示如下：通过上图说明系统无法重启，打电话给阿里云客服，客服建议提交工单找阿里工程师处理（工单受理等待时间一般为半小时），工程师尝试恢复系统也无果，给出如下处理建议。 尝试了各种方法暂时无法解决这个问题。建议做好备份，然后重新初始化磁盘，然后重新安装应用恢复业务，加强主机安全。创建快照:https://help.aliyun.com/document_detail/25455.html?spm=5176.product25365.6.701.haklDS重新初始化磁盘 https://help.aliyun.com/document_detail/25449.html?spm=5176.doc25455.6.697.ywlUna重新初始化后如需要之前系统的数据，可以使用快照创建云盘，然后挂载到数据，然后拷贝出需要的数据。https://help.aliyun.com/document_detail/32317.html?spm=5176.doc25455.6.680.JDavY 由于服务器上有重要数据包括应用部署文件及mysql数据库文件，之前都没有备份，所以接下来的每一步操作都慎之又慎，心里默念数据不能丢。具体操作步骤如下： 对系统盘创建快照这个比较耗时，系统盘空间20G创建快照大概用时20分钟左右（直接在阿里云界面操作）。 创建云盘并保存系统盘快照创建云盘（阿里云云盘现在开始收费，按时收费份SSD盘和普通盘价格不一样，SSD盘20G每小时0.028元），将快照导入云盘（直接在阿里云界面操作）。创建云盘选择地区时一定要注意选择与ECS服务地区一致，否则到时候无法挂载。 系统盘格式化点击重新初始化磁盘（直接在阿里云界面操作），做了这个操作相当于对ECS实例进行了恢复出厂设置操作 启动实例直接在界面操作 ssh远程连接使用ssh远程工具进行连接，输入密码登录成功，一个全新ESC服务器诞生了 挂载云盘直接在阿里云界面操作，挂载云盘后，在linux系统中就会发现一个新的空间，挂载盘作为系统盘中根目录下的一个文件夹，文件夹中的内容就是之前系统盘中数据，目录结构也与之前系统盘中的目录保持一致。 配置jdk直接在挂载盘中找到之前部署的应用，启动tomcat无法启动，报错找不到jdk，在/etc/profile下配置jdk环境变量并让jdk生效。1vim /etc/profile 在/etc/profile文件最底部添加jdk环境变量的配置1234export JAVA_HOME=/mnt/usr/java/jdk1.7.0_79export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/binexport JAVA_HOME CLASSPATH PATH 使配置立即生效1source /etc/profile mysql数据库迁移先安装数据库1yum install -y mysql-server mysql mysql-devel 老数据迁移 在/var/lib/mysql目录下 执行如下数据，将老数据迁移到新安装的数据库，迁移是注意，对本目录下ibdata1进行备份，防止迁移迁移过来后数据库服务不能启动123cp /mnt/var/lib/mysql/ibdata1 ./ -f ##这是保存的数据cp -rf /mnt/var/lib/mysql/databasename ./ ##这是要迁移的数据库目录 与数据库名同名sync 启动mysql服务，并登录mysql 查询数据是否迁移成功。 至此，数据库迁移成功，jdk环境变量配置成功，tomcat服务重新启动，大功告成！ 结语本次处理问题环境 环境 版本 系统 linux Centos6.5 mysql 5.1.73 jdk build 1.7.0_79-b15 tomcat 7.0.67 此次最担心的是数据丢失，告诫自己，以后在生产环境必须对数据库进行备份或做数据实时同步，防止数据丢失。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
        <tag>阿里云</tag>
        <tag>centos6.5</tag>
        <tag>数据迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ContextLoaderListener初始化的前后文和DispatcherServlet初始化的上下文关系]]></title>
    <url>%2F2017%2F09%2F11%2FContextLoaderListener%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E5%89%8D%E5%90%8E%E6%96%87%E5%92%8CDispatcherServlet%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[从图中可以看出： ContextLoaderListener初始化的上下文加载的Bean是对于整个应用程序共享的，不管是使用什么表现层技术，一般如DAO层、Service层Bean； DispatcherServlet初始化的上下文加载的Bean是只对Spring Web MVC有效的Bean，如Controller、HandlerMapping、HandlerAdapter等等，该初始化上下文应该只加载Web相关组件。 如果分开配置，则web.xml如下图1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app version="3.1" xmlns="http://xmlns.jcp.org/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/applicationContext.xml&lt;/param-value&gt; &lt;!-- 默认是/WEB-INF/applicationContext.xml --&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/SpringMVC-servlet.xml&lt;/param-value&gt; &lt;!-- 默认是/WEB-INF/[servlet名字]-servlet.xml --&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springmvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot集成swagger2]]></title>
    <url>%2F2017%2F08%2F24%2Fspring-boot%E9%9B%86%E6%88%90swagger2%2F</url>
    <content type="text"><![CDATA[1.pom.xml引入swagger2依赖：1234567891011&lt;!--Swagger2的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;$&#123;swagger.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2.代码配置12345678910111213141516171819202122232425262728293031323334353637383940@Configuration@EnableSwagger2public class Swagger2Config extends WebMvcConfigurerAdapter &#123; /** * 这个地方要重新注入一下资源文件，不然不会注入资源的，也没有注入requestHandlerMappping,相当于xml配置的 * &lt;!--swagger资源配置--&gt; * &lt;mvc:resources location="classpath:/META-INF/resources/" mapping="swagger-ui.html"/&gt; * &lt;mvc:resources location="classpath:/META-INF/resources/webjars/" mapping="/webjars/**"/&gt; * 不知道为什么，这也是spring boot的一个缺点（菜鸟觉得的） * @param registry */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler("swagger-ui.html") .addResourceLocations("classpath:/META-INF/resources/"); registry.addResourceHandler("/webjars*") .addResourceLocations("classpath:/META-INF/resources/webjars/"); &#125; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage("com.kiko.controller")) //需要扫描的路径 .paths(PathSelectors.any()) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title("Spring Boot中使用Swagger2构建RESTful APIs") .description("更多Spring Boot相关文章请关注：http://www.leiyatao.com/") .termsOfServiceUrl("http://www.leiyatao.com/") .contact("leiyatao0204@qq.com") .version("1.0").license("2.0").licenseUrl("http://www.leiyatao.com") .build(); &#125;&#125; 3.Controller方法中添加注解1234567891011121314151617181920212223242526272829303132333435363738394041424344@RestController@Api(value = "用户接口", description = "描述")public class HelloWorldController &#123; @Autowired private UserRepository userRepository; @RequestMapping(value="hello",method=RequestMethod.GET) public String hello()&#123; return "Hello Spring boot"; &#125; @ApiOperation(value="获取用户详细信息", notes="根据url的id来获取用户详细信息") @ApiImplicitParam(name = "id", value = "用户ID", required = true,dataType = "Long",paramType = "query") @RequestMapping(value="getUser", method=RequestMethod.GET) public User getUserById(Long id) &#123; return userRepository.getOne(id); &#125; @ApiOperation(value="创建用户", notes="根据User对象创建用户") @ApiImplicitParam(name = "user", value = "用户详细实体user", required = true, dataType = "User" ) @RequestMapping(value="saveUser", method=RequestMethod.POST) public String saveUser(@RequestBody User user)&#123; userRepository.save(user); return "success"; &#125; @ApiOperation(value="修改用户", notes="根据User对象修改用户") @ApiImplicitParam(name = "user", value = "用户详细实体user", required = true, dataType = "User" ) @RequestMapping(value="updateUser", method=RequestMethod.POST) public String updateUser(@RequestBody User user)&#123; User user1=userRepository.getOne(user.getId()); user1.setAge(user.getAge()); user1.setName(user.getName()); userRepository.save(user1); return "success"; &#125; @ApiOperation(value="查询所有", notes="查询所有") @RequestMapping(value="findAll", method=RequestMethod.POST) public List&lt;User&gt; findAll()&#123; return userRepository.findAll(); &#125;&#125; 4.启动工程并访问http://localhost:9999/swagger-ui.html大功告成！！]]></content>
      <categories>
        <category>Spring-boot</category>
      </categories>
      <tags>
        <tag>Spring-boot</tag>
        <tag>swagger2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper+dubbo环境搭建]]></title>
    <url>%2F2017%2F06%2F27%2Fzookeeper-dubbo%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[安装前准备 jdk1.7+ 下载地址 apache-tomcat-7.0.67.tar.gz 下载地址 dubbo-admin-2.5.4.war 下载地址 zookeeper-3.4.9 .tar.gz 下载地址 zookeeper下载 地址:http://apache.fayea.com/zookeeper/ 上传本地文件到远程服务器123scp zookeeper-3.4.9.tar.gz root@remote:/reyesscp dubbo-admin-2.5.4.war root@remote:/reyes 安装安装配置jdk这个网上大把,自己找 安装zookeeper解压1tar -xvf zookeeper-3.4.9.tar.gz 配置在解压后的文件中找到zookeeper-3.4.9/conf/,这里有三个文件123456/reyes/zookeeper-3.4.9/conf[root@iZ948994a6hZ conf]# lltotal 12-rw-rw-r-- 1 1001 1001 535 Aug 23 2016 configuration.xsl-rw-rw-r-- 1 1001 1001 2161 Aug 23 2016 log4j.properties-rw-rw-r-- 1 1001 1001 922 Aug 23 2016 zoo_sample.cfg 复制其中的zoo_sample.cfg重命名为zoo.cfg1cp zoo_sample.cfg zoo.cfg 修改zoo.cfg123456tickTime=2000initLimit=10syncLimit=5dataDir=../datadataLogDir=../logsclientPort=2181 启动在bin目录启动zookeeper服务1./zkServer.sh start dubbo-admin部署 解压tomcat 删除tomcat webapp底下的所有文件夹,只保留ROOT目录,清理ROOT目录 将dubbo-admin-2.5.4.war复制到 tomcat的ROOT目录下 启动tomcat服务 访问浏览器dubbo-admin访问 http://localhost:8080进入 dubbo-admin登录界面,输入root/root 进入管理界面则大功告成 注意事项dubbo-admin与jdk及tomcat兼容问题,我在远程服务器部署时,远程服务器是tomcat7,jdk1.7 在部署dubbo-admin-2.5.3时没有问题 在本地mac电脑中进行dubbo部署,由于本地安装的jdk1.8 使用tomcat8.5部署,启动tomcat时报错,后在网上找了各种资料,说2.5.3与jdk8/tomcat8不兼容,后下载dubbo-admin-2.5.4.war重新部署后OK]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>dobbo</tag>
        <tag>rpc</tag>
        <tag>远程接口调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令检查当前文件夹子目录所占空间大小]]></title>
    <url>%2F2017%2F06%2F19%2Flinux%E5%91%BD%E4%BB%A4%E6%A3%80%E6%9F%A5%E5%BD%93%E5%89%8D%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AD%90%E7%9B%AE%E5%BD%95%E6%89%80%E5%8D%A0%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[今天开发服务器[linux]上的mongodb数据库突然挂了,打开mongodb日志发现如下提示： 12345678910111213141516172017-06-19T15:32:51.629+0800 [initandlisten] ERROR: Insufficient free space for journal files2017-06-19T15:32:51.629+0800 [initandlisten] Please make at least 3379MB available in ../data/journal or use --smallfiles2017-06-19T15:32:51.629+0800 [initandlisten] 2017-06-19T15:32:51.641+0800 [initandlisten] exception in initAndListen: 15926 Insufficient free space for journals, terminating2017-06-19T15:32:51.641+0800 [initandlisten] dbexit: 2017-06-19T15:32:51.641+0800 [initandlisten] shutdown: going to close listening sockets...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: going to flush diaglog...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: going to close sockets...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: waiting for fs preallocator...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: lock for final commit...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: final commit...2017-06-19T15:32:51.642+0800 [initandlisten] shutdown: closing all files...2017-06-19T15:32:51.642+0800 [initandlisten] closeAllFiles() finished2017-06-19T15:32:51.642+0800 [initandlisten] journalCleanup...2017-06-19T15:32:51.642+0800 [initandlisten] removeJournalFiles2017-06-19T15:32:51.643+0800 [initandlisten] shutdown: removing fs lock...2017-06-19T15:32:51.643+0800 [initandlisten] dbexit: really exiting now 从日志文件明显看出，linux服务器磁盘空间不足。使用df-hl命令查看磁盘空间发现果然如此,/opt挂载盘可用空间为0123456789[root@hs010 proc]# df -hlFilesystem Size Used Avail Use% Mounted on/dev/mapper/sysvg-rootlv 4.8G 2.4G 2.2G 53% /tmpfs 3.9G 0 3.9G 0% /dev/shm/dev/sda1 477M 55M 398M 12% /boot/dev/mapper/sysvg-homelv 4.8G 10M 4.6G 1% /home/dev/mapper/sysvg-optlv 40G 38G 0 100% /opt/dev/mapper/sysvg-tmplv 9.8G 86M 9.2G 1% /tmp/dev/mapper/sysvg-varlv 9.8G 766M 8.5G 9% /var 通过find . -type f -size +800M -print0 | xargs -0 ls -l命令查找大于800M的文件发现 zookeeper.out日志文件比较大。如下：1234567[root@hs010 /]# find . -type f -size +800M -print0 | xargs -0 ls -lfind: `./proc/3677/task/3677/fd/5&apos;: No such file or directoryfind: `./proc/3677/task/3677/fdinfo/5&apos;: No such file or directoryfind: `./proc/3677/fd/5&apos;: No such file or directoryfind: `./proc/3677/fdinfo/5&apos;: No such file or directory-rw-r--r-- 1 root root 991461376 Jun 18 05:46 ./opt/pcserv/zookeeper/bin/zookeeper.out-r-------- 1 root root 140737486266368 Jun 19 16:03 ./proc/kcore 找到大文件删除后，重启mongodb服务ok 另外还可用du命令进行当前目录下子目录所占空间的查询12345678du -h --max-depth=1 |grep [TG] |sort #查找上G和T的目录并排序du -sh #统计当前目录的大小，以直观方式展现du -h --max-depth=1 |grep 'G' |sort #查看上G目录并排序du -sh --max-depth=1 #查看当前目录下所有一级子目录文件夹大小du -h --max-depth=1 |sort #查看当前目录下所有一级子目录文件夹大小 并排序du -h --max-depth=1 |grep [TG] |sort -nr #倒序排]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL函数总汇]]></title>
    <url>%2F2017%2F06%2F08%2FMySQL%E5%87%BD%E6%95%B0%E6%80%BB%E6%B1%87%2F</url>
    <content type="text"><![CDATA[前言 MySQL提供了众多功能强大、方便易用的函数，使用这些函数，可以极大地提高用户对于数据库的管理效率，从而更加灵活地满足不同用户的需求。本文将MySQL的函数分类并汇总，以便以后用到的时候可以随时查看。 ##数学函数 ABS(x) 返回x的绝对值 PI() 返回圆周率π，默认显示6位小数 SQRT(x) 返回非负数的x的二次方根 MOD(x,y) 返回x被y除后的余数 CEIL(x)、CEILING(x) 返回不小于x的最小整数 FLOOR(x) 返回不大于x的最大整数 ROUND(x)、ROUND(x,y) 前者返回最接近于x的整数，即对x进行四舍五入；后者返回最接近x的数，其值保留到小数点后面y位，若y为负值，则将保留到x到小数点左边y位 SIGN(x) 返回参数x的符号，-1表示负数，0表示0，1表示正数 POW(x,y)和、POWER(x,y) 返回x的y次乘方的值 EXP(x) 返回e的x乘方后的值 LOG(x) 返回x的自然对数，x相对于基数e的对数 LOG10(x) 返回x的基数为10的对数 RADIANS(x) 返回x由角度转化为弧度的值 DEGREES(x) 返回x由弧度转化为角度的值 SIN(x)、ASIN(x) 前者返回x的正弦，其中x为给定的弧度值；后者返回x的反正弦值，x为正弦 COS(x)、ACOS(x) 前者返回x的余弦，其中x为给定的弧度值；后者返回x的反余弦值，x为余弦 TAN(x)、ATAN(x) 前者返回x的正切，其中x为给定的弧度值；后者返回x的反正切值，x为正切 COT(x) 返回给定弧度值x的余切 字符串函数CHAR_LENGTH(str) 计算字符串字符个数 CONCAT(s1,s2，…) 返回连接参数产生的字符串，一个或多个待拼接的内容，任意一个为NULL则返回值为NULL CONCAT_WS(x,s1,s2,…) 返回多个字符串拼接之后的字符串，每个字符串之间有一个x INSERT(s1,x,len,s2) 返回字符串s1，其子字符串起始于位置x，被字符串s2取代len个字符 LOWER(str)和LCASE(str)、UPPER(str)和UCASE(str) 前两者将str中的字母全部转换成小写，后两者将字符串中的字母全部转换成大写 LEFT(s,n)、RIGHT(s,n) 前者返回字符串s从最左边开始的n个字符，后者返回字符串s从最右边开始的n个字符 LPAD(s1,len,s2)、RPAD(s1,len,s2) 前者返回s1，其左边由字符串s2填补到len字符长度，假如s1的长度大于len，则返回值被缩短至len字符；前者返回s1，其右边由字符串s2填补到len字符长度，假如s1的长度大于len，则返回值被缩短至len字符 LTRIM(s)、RTRIM(s) 前者返回字符串s，其左边所有空格被删除；后者返回字符串s，其右边所有空格被删除 TRIM(s) 返回字符串s删除了两边空格之后的字符串 TRIM(s1 FROM s) 删除字符串s两端所有子字符串s1，未指定s1的情况下则默认删除空格 REPEAT(s,n) 返回一个由重复字符串s组成的字符串，字符串s的数目等于n SPACE(n) 返回一个由n个空格组成的字符串 REPLACE(s,s1,s2) 返回一个字符串，用字符串s2替代字符串s中所有的字符串s1 STRCMP(s1,s2) 若s1和s2中所有的字符串都相同，则返回0；根据当前分类次序，第一个参数小于第二个则返回-1，其他情况返回1 SUBSTRING(s,n,len)、MID(s,n,len) 两个函数作用相同，从字符串s中返回一个第n个字符开始、长度为len的字符串 LOCATE(str1,str)、POSITION(str1 IN str)、INSTR(str,str1) 三个函数作用相同，返回子字符串str1在字符串str中的开始位置（从第几个字符开始） REVERSE(s) 将字符串s反转 ELT(N,str1,str2,str3,str4,…) 返回第N个字符串 日期和时间函数CURDATE()、CURRENT_DATE() 将当前日期按照”YYYY-MM-DD”或者”YYYYMMDD”格式的值返回，具体格式根据函数用在字符串或是数字语境中而定 CURRENT_TIMESTAMP()、LOCALTIME()、NOW()、SYSDATE() 这四个函数作用相同，返回当前日期和时间值，格式为”YYYY_MM-DD HH:MM:SS”或”YYYYMMDDHHMMSS”，具体格式根据函数用在字符串或数字语境中而定 UNIX_TIMESTAMP()、UNIX_TIMESTAMP(date) 前者返回一个格林尼治标准时间1970-01-01 00:00:00到现在的秒数，后者返回一个格林尼治标准时间1970-01-01 00:00:00到指定时间的秒数 FROM_UNIXTIME(date) 和UNIX_TIMESTAMP互为反函数，把UNIX时间戳转换为普通格式的时间 UTC_DATE()和UTC_TIME() 前者返回当前UTC（世界标准时间）日期值，其格式为”YYYY-MM-DD”或”YYYYMMDD”，后者返回当前UTC时间值，其格式为”YYYY-MM-DD”或”YYYYMMDD”。具体使用哪种取决于函数用在字符串还是数字语境中 MONTH(date)和MONTHNAME(date) 前者返回指定日期中的月份，后者返回指定日期中的月份的名称 DAYNAME(d)、DAYOFWEEK(d)、WEEKDAY(d) DAYNAME(d)返回d对应的工作日的英文名称，如Sunday、Monday等；DAYOFWEEK(d)返回的对应一周中的索引，1表示周日、2表示周一；WEEKDAY(d)表示d对应的工作日索引，0表示周一，1表示周二 WEEK(d)、WEEKOFYEAD(d) 前者计算日期d是一年中的第几周，后者计算某一天位于一年中的第几周 DAYOFYEAR(d)、DAYOFMONTH(d) 前者返回d是一年中的第几天，后者返回d是一月中的第几天 YEAR(date)、QUARTER(date)、MINUTE(time)、SECOND(time) YEAR(date)返回指定日期对应的年份，范围是1970~2069；QUARTER(date)返回date对应一年中的季度，范围是1~4；MINUTE(time)返回time对应的分钟数，范围是0~59；SECOND(time)返回制定时间的秒值 EXTRACE(type FROM date) 从日期中提取一部分，type可以是YEAR、YEAR_MONTH、DAY_HOUR、DAY_MICROSECOND、DAY_MINUTE、DAY_SECOND TIME_TO_SEC(time) 返回以转换为秒的time参数，转换公式为”3600小时 + 60分钟 + 秒” SEC_TO_TIME() 和TIME_TO_SEC(time)互为反函数，将秒值转换为时间格式 DATE_ADD(date,INTERVAL expr type)、ADD_DATE(date,INTERVAL expr type) 返回将起始时间加上expr type之后的时间，比如DATE_ADD(‘2010-12-31 23:59:59’, INTERVAL 1 SECOND)表示的就是把第一个时间加1秒 DATE_SUB(date,INTERVAL expr type)、SUBDATE(date,INTERVAL expr type) 返回将起始时间减去expr type之后的时间 ADDTIME(date,expr)、SUBTIME(date,expr) 前者进行date的时间加操作，后者进行date的时间减操作 条件判断函数IF(expr,v1,v2) 如果expr是TRUE则返回v1，否则返回v2 IFNULL(v1,v2) 如果v1不为NULL，则返回v1，否则返回v2 CASE expr WHEN v1 THEN r1 [WHEN v2 THEN v2] [ELSE rn] END 如果expr等于某个vn，则返回对应位置THEN后面的结果，如果与所有值都不想等，则返回ELSE后面的rn 系统信息函数VERSION() 查看MySQL版本号 CONNECTION_ID() 查看当前用户的连接数 USER()、CURRENT_USER()、SYSTEM_USER()、SESSION_USER() 查看当前被MySQL服务器验证的用户名和主机的组合，一般这几个函数的返回值是相同的 CHARSET(str) 查看字符串str使用的字符集 COLLATION(str) 查看字符串排列方式 加密函数PASSWORD(str) 从原明文密码str计算并返回加密后的字符串密码，注意这个函数的加密是单向的（不可逆），因此不应将它应用在个人的应用程序中而应该只在MySQL服务器的鉴定系统中使用 MD5(str) 为字符串算出一个MD5 128比特校验和，改值以32位十六进制数字的二进制字符串形式返回 ENCODE(str, pswd_str) 使用pswd_str作为密码，加密str DECODE(crypt_str,pswd_str) 使用pswd_str作为密码，解密加密字符串crypt_str，crypt_str是由ENCODE函数返回的字符串 其他函数FORMAT(x,n) 将数字x格式化，并以四舍五入的方式保留小数点后n位，结果以字符串形式返回 CONV(N,from_base,to_base) 不同进制数之间的转换，返回值为数值N的字符串表示，由from_base进制转换为to_base进制 INET_ATON(expr) 给出一个作为字符串的网络地址的点地址表示，返回一个代表该地址数值的整数，地址可以使4或8比特 INET_NTOA(expr) 给定一个数字网络地址（4或8比特），返回作为字符串的该地址的点地址表示 BENCHMARK(count,expr) 重复执行count次表达式expr，它可以用于计算MySQL处理表达式的速度，结果值通常是0（0只是表示很快，并不是没有速度）。另一个作用是用它在MySQL客户端内部报告语句执行的时间 CONVERT(str USING charset) 使用字符集charset表示字符串str]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
        <tag>数据库</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORACLE语句整理]]></title>
    <url>%2F2017%2F05%2F25%2FORACLE%E6%9F%A5%E7%9C%8B%E6%89%80%E6%9C%89%E8%A1%A8%E5%92%8C%E5%AD%97%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[表备份1234-- 备份表结构和数据create table table_name_new as select * from table_name_old where 1=1;-- 备份空表结构create table table_name_new as select * from table_name_old where 1=2; 查看当前用户下所有表1select table_name from user_tables; 查看当前用户下所有表及表注释1select * from user_tab_comments; 获取表字段：1select * from user_tab_columns where Table_Name='用户表' order by column_name 获取表注释：1select * from user_tab_comments where Table_Name='用户表' order by Table_Name 获取字段注释：1select * from user_col_comments where Table_Name='用户表' order by column_name]]></content>
      <categories>
        <category>ORACLE</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>ORACLE</tag>
        <tag>数据备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魅力甘南]]></title>
    <url>%2F2016%2F06%2F25%2F%E9%AD%85%E5%8A%9B%E7%94%98%E5%8D%97%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>旅行</category>
      </categories>
      <tags>
        <tag>旅行</tag>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[告别2014]]></title>
    <url>%2F2015%2F01%2F01%2F%E5%91%8A%E5%88%AB2014%2F</url>
    <content type="text"><![CDATA[用照片回顾这一年，告别的是一个个地方，也是一段段属于自己的生命。 春节在家睡大觉，睡到初三。没憋住去了趟厦门，城市旅行就是换个地方继续睡大觉。 环岛路上 沙画公园 摩天轮，给喜欢的人 成都，来过无数次的地方。工作后来成都的次数超过回家的次数。 都江堰，那年地震过后，看过她的满目疮痍，今年来过两次，虽然已重建多年，但处处还有地震留下的累累伤痕。 重庆，七月出差来这里，酒店大堂在一楼，停车场在八路，直通酒店背后大马路 色达，一个向往了很多年的地方，路途充满各种艰辛也乐在其中。 历尽艰险，就是为了多看你一眼 老天总会眷顾善良的孩子（大叔） 塔公草原，那年在新都桥与你擦肩而过时，三年后我们终于相遇 雅拉雪山 香港，从十一月初，每个周末都去，我不生产奶，也不做奶粉的搬运工 来年，依旧不断看见世界，不断跨越藩篱，感受内心切近生活，为了远方的风和传说，继续脚下漫长的旅途……]]></content>
      <categories>
        <category>旅行</category>
      </categories>
      <tags>
        <tag>旅行</tag>
        <tag>摄影</tag>
      </tags>
  </entry>
</search>
